#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è”åˆMLM+å¯¹æ¯”å­¦ä¹ BERTæ¨¡å‹è®­ç»ƒè„šæœ¬

å®ç°è®ºæ–‡ä¸­çš„è”åˆä¼˜åŒ–æ¡†æ¶ï¼Œç”¨äºæ— çº¿é€šä¿¡é¢†åŸŸçš„åµŒå…¥æ¨¡å‹å¾®è°ƒ
"""

import os
import json
import argparse
import logging
import random
import numpy as np
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import time
import copy

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import Dataset, DataLoader, random_split
from torch.utils.data.distributed import DistributedSampler
from transformers import (
    AutoTokenizer, 
    get_linear_schedule_with_warmup,
    set_seed
)
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
import pandas as pd
from tqdm import tqdm

from joint_bert_model import JointBertModel, JointBertTrainer
from tokenizer_utils import get_enhanced_tokenizer, get_extended_embeddings

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class EarlyStopping:
    """æ—©åœæœºåˆ¶ç±»"""
    
    def __init__(self, patience=7, min_delta=0.001, restore_best_weights=True):
        """
        åˆå§‹åŒ–æ—©åœæœºåˆ¶
        
        Args:
            patience: å®¹å¿æ²¡æœ‰æ”¹å–„çš„epochæ•°
            min_delta: æœ€å°æ”¹å–„é˜ˆå€¼
            restore_best_weights: æ˜¯å¦æ¢å¤æœ€ä½³æƒé‡
        """
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        
        self.best_loss = float('inf')
        self.counter = 0
        self.best_weights = None
        self.early_stop = False
        
    def __call__(self, val_loss, model):
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ
        
        Args:
            val_loss: å½“å‰éªŒè¯æŸå¤±
            model: å½“å‰æ¨¡å‹
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥æ—©åœ
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹å–„
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            if self.restore_best_weights:
                self.best_weights = copy.deepcopy(model.state_dict())
        else:
            self.counter += 1
            
        if self.counter >= self.patience:
            self.early_stop = True
            logger.info(f"ğŸ›‘ æ—©åœè§¦å‘ï¼éªŒè¯æŸå¤±åœ¨ {self.patience} ä¸ªepochå†…æ²¡æœ‰æ”¹å–„")
            if self.restore_best_weights and self.best_weights is not None:
                model.load_state_dict(self.best_weights)
                logger.info("ğŸ”„ å·²æ¢å¤æœ€ä½³æƒé‡")
        
        return self.early_stop

class GradientNormTracker:
    """æ¢¯åº¦èŒƒæ•°è¿½è¸ªå™¨"""
    
    def __init__(self, window_size=100):
        self.window_size = window_size
        self.gradient_norms = []
        
    def update(self, model):
        """æ›´æ–°æ¢¯åº¦èŒƒæ•°"""
        total_norm = 0
        param_count = 0
        
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1
        
        if param_count > 0:
            total_norm = total_norm ** (1. / 2)
            self.gradient_norms.append(total_norm)
            
            # ä¿æŒçª—å£å¤§å°
            if len(self.gradient_norms) > self.window_size:
                self.gradient_norms.pop(0)
    
    def get_stats(self):
        """è·å–æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯"""
        if not self.gradient_norms:
            return {}
        
        return {
            'mean_grad_norm': np.mean(self.gradient_norms),
            'max_grad_norm': np.max(self.gradient_norms),
            'min_grad_norm': np.min(self.gradient_norms),
            'std_grad_norm': np.std(self.gradient_norms)
        }

class MLMDataset(Dataset):
    """MLMæ•°æ®é›†"""
    
    def __init__(self, 
                 data_path: str, 
                 tokenizer: AutoTokenizer,
                 max_length: int = 128):
        """
        åˆå§‹åŒ–MLMæ•°æ®é›†
        
        Args:
            data_path: MLMæ•°æ®CSVæ–‡ä»¶è·¯å¾„
            tokenizer: åˆ†è¯å™¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv(data_path)
        self.samples = []
        
        for _, row in df.iterrows():
            original_text = str(row['original_text'])
            masked_text = str(row['masked_text'])
            
            # è§£ææ ‡ç­¾
            try:
                mask_labels = eval(row['mask_labels']) if isinstance(row['mask_labels'], str) else row['mask_labels']
            except:
                mask_labels = []
            
            self.samples.append({
                'original_text': original_text,
                'masked_text': masked_text,
                'mask_labels': mask_labels,
                'source_file': row.get('source_file', 'unknown')
            })
        
        logger.info(f"åŠ è½½äº† {len(self.samples)} ä¸ªMLMæ ·æœ¬")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # ç¼–ç æ©ç æ–‡æœ¬
        encoding = self.tokenizer(
            sample['masked_text'],
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # åˆ›å»ºMLMæ ‡ç­¾
        input_ids = encoding['input_ids'].squeeze()
        labels = input_ids.clone()
        
        # æ‰¾åˆ°[MASK]ä½ç½®å¹¶è®¾ç½®æ ‡ç­¾
        mask_token_id = self.tokenizer.mask_token_id
        mask_positions = (input_ids == mask_token_id).nonzero(as_tuple=True)[0]
        
        # å°†éæ©ç ä½ç½®çš„æ ‡ç­¾è®¾ä¸º-100ï¼ˆå¿½ç•¥ï¼‰
        labels[:] = -100
        
        # ä¸ºæ©ç ä½ç½®è®¾ç½®çœŸå®æ ‡ç­¾ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®åŸå§‹æ–‡æœ¬ç¡®å®šï¼‰
        # ç”±äºæˆ‘ä»¬çš„æ•°æ®ç”Ÿæˆæ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„å¤„ç†æ–¹å¼
        for pos in mask_positions:
            # éšæœºé€‰æ‹©ä¸€ä¸ªé€šä¿¡é¢†åŸŸè¯æ±‡ä½œä¸ºæ ‡ç­¾ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥æ ¹æ®åŸå§‹æ–‡æœ¬çš„å¯¹åº”ä½ç½®ç¡®å®šçœŸå®æ ‡ç­¾
            if sample['mask_labels']:
                # å¦‚æœæœ‰æ ‡ç­¾ä¿¡æ¯ï¼Œå°è¯•ä½¿ç”¨
                try:
                    original_word = random.choice([label['original_word'] for label in sample['mask_labels']])
                    token_id = self.tokenizer.convert_tokens_to_ids(original_word)
                    if token_id != self.tokenizer.unk_token_id:
                        labels[pos] = token_id
                    else:
                        labels[pos] = input_ids[pos]  # ä¿æŒåŸæ ·
                except:
                    labels[pos] = input_ids[pos]
            else:
                labels[pos] = input_ids[pos]
        
        return {
            'input_ids': input_ids,
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': labels
        }

class ContrastiveDataset(Dataset):
    """å¯¹æ¯”å­¦ä¹ æ•°æ®é›†"""
    
    def __init__(self, 
                 data_path: str, 
                 tokenizer: AutoTokenizer,
                 max_length: int = 128):
        """
        åˆå§‹åŒ–å¯¹æ¯”å­¦ä¹ æ•°æ®é›†
        
        Args:
            data_path: å¯¹æ¯”å­¦ä¹ æ•°æ®CSVæ–‡ä»¶è·¯å¾„
            tokenizer: åˆ†è¯å™¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv(data_path)
        self.samples = []
        
        for _, row in df.iterrows():
            self.samples.append({
                'text1': str(row['text1']),
                'text2': str(row['text2']),
                'label': int(row['label']),
                'context1': str(row.get('context1', row['text1'])),
                'context2': str(row.get('context2', row['text2'])),
                'pair_type': str(row.get('pair_type', 'unknown'))
            })
        
        logger.info(f"åŠ è½½äº† {len(self.samples)} ä¸ªå¯¹æ¯”å­¦ä¹ æ ·æœ¬")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # ç¼–ç ç¬¬ä¸€ä¸ªæ–‡æœ¬ï¼ˆä½¿ç”¨contextä»¥è·å¾—æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼‰
        encoding1 = self.tokenizer(
            sample['context1'],
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # ç¼–ç ç¬¬äºŒä¸ªæ–‡æœ¬
        encoding2 = self.tokenizer(
            sample['context2'],
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids_1': encoding1['input_ids'].squeeze(),
            'attention_mask_1': encoding1['attention_mask'].squeeze(),
            'input_ids_2': encoding2['input_ids'].squeeze(),
            'attention_mask_2': encoding2['attention_mask'].squeeze(),
            'labels': torch.tensor(sample['label'], dtype=torch.long)
        }

def collate_mlm_batch(batch):
    """MLMæ‰¹æ¬¡æ•´ç†å‡½æ•°"""
    return {
        'input_ids': torch.stack([item['input_ids'] for item in batch]),
        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),
        'labels': torch.stack([item['labels'] for item in batch])
    }

def collate_contrastive_batch(batch):
    """å¯¹æ¯”å­¦ä¹ æ‰¹æ¬¡æ•´ç†å‡½æ•°"""
    return {
        'input_ids_1': torch.stack([item['input_ids_1'] for item in batch]),
        'attention_mask_1': torch.stack([item['attention_mask_1'] for item in batch]),
        'input_ids_2': torch.stack([item['input_ids_2'] for item in batch]),
        'attention_mask_2': torch.stack([item['attention_mask_2'] for item in batch]),
        'labels': torch.stack([item['labels'] for item in batch])
    }

class JointTrainingManager:
    """è”åˆè®­ç»ƒç®¡ç†å™¨"""
    
    def __init__(self, 
                 model: JointBertModel,
                 mlm_dataloader: DataLoader,
                 contrastive_dataloader: DataLoader,
                 optimizer: torch.optim.Optimizer,
                 scheduler: Optional[torch.optim.lr_scheduler._LRScheduler],
                 device: str,
                 beta_ft: float = 0.5,
                 use_distributed: bool = False,
                 mlm_sampler: Optional[DistributedSampler] = None,
                 contrastive_sampler: Optional[DistributedSampler] = None,
                 val_mlm_dataloader: Optional[DataLoader] = None,
                 val_contrastive_dataloader: Optional[DataLoader] = None,
                 early_stopping: Optional[EarlyStopping] = None,
                 use_dynamic_lr: bool = False,
                 use_dynamic_beta: bool = False):
        """
        åˆå§‹åŒ–è®­ç»ƒç®¡ç†å™¨
        
        Args:
            model: è”åˆBERTæ¨¡å‹
            mlm_dataloader: MLMæ•°æ®åŠ è½½å™¨
            contrastive_dataloader: å¯¹æ¯”å­¦ä¹ æ•°æ®åŠ è½½å™¨
            optimizer: ä¼˜åŒ–å™¨
            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
            device: è®¾å¤‡
            beta_ft: è”åˆæŸå¤±æƒé‡å‚æ•°
            use_distributed: æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
            mlm_sampler: MLMåˆ†å¸ƒå¼é‡‡æ ·å™¨
            contrastive_sampler: å¯¹æ¯”å­¦ä¹ åˆ†å¸ƒå¼é‡‡æ ·å™¨
            val_mlm_dataloader: éªŒè¯MLMæ•°æ®åŠ è½½å™¨
            val_contrastive_dataloader: éªŒè¯å¯¹æ¯”å­¦ä¹ æ•°æ®åŠ è½½å™¨
            early_stopping: æ—©åœæœºåˆ¶
            use_dynamic_lr: æ˜¯å¦ä½¿ç”¨åŠ¨æ€å­¦ä¹ ç‡
            use_dynamic_beta: æ˜¯å¦ä½¿ç”¨åŠ¨æ€beta_ft
        """
        self.model = model
        self.mlm_dataloader = mlm_dataloader
        self.contrastive_dataloader = contrastive_dataloader
        self.val_mlm_dataloader = val_mlm_dataloader
        self.val_contrastive_dataloader = val_contrastive_dataloader
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        self.beta_ft = beta_ft
        self.initial_beta_ft = beta_ft  # ä¿å­˜åˆå§‹å€¼
        self.use_distributed = use_distributed
        self.mlm_sampler = mlm_sampler
        self.contrastive_sampler = contrastive_sampler
        self.early_stopping = early_stopping
        self.use_dynamic_lr = use_dynamic_lr
        self.use_dynamic_beta = use_dynamic_beta
        
        # åˆ›å»ºè®­ç»ƒå™¨
        self.trainer = JointBertTrainer(model, beta_ft, device)
        
        # åˆ›å»ºåŠ¨æ€å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.use_dynamic_lr:
            self.lr_scheduler = ReduceLROnPlateau(
                self.optimizer, 
                mode='min', 
                factor=0.5, 
                patience=3,
                verbose=True,
                min_lr=1e-7
            )
        
        # æ¢¯åº¦èŒƒæ•°è¿½è¸ªå™¨
        self.grad_tracker = GradientNormTracker()
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'epoch_losses': [],
            'mlm_losses': [],
            'contrastive_losses': [],
            'joint_losses': [],
            'val_losses': [],
            'learning_rates': [],
            'beta_ft_history': [],
            'gradient_stats': []
        }
    
    def validate(self) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹"""
        if self.val_mlm_dataloader is None or self.val_contrastive_dataloader is None:
            return {}
        
        self.model.eval()
        val_mlm_loss = 0.0
        val_contrastive_loss = 0.0
        val_joint_loss = 0.0
        total_steps = 0
        
        with torch.no_grad():
            # åˆ›å»ºéªŒè¯æ•°æ®è¿­ä»£å™¨
            val_mlm_iter = iter(self.val_mlm_dataloader)
            val_contrastive_iter = iter(self.val_contrastive_dataloader)
            
            # è®¡ç®—æ­¥æ•°
            steps = min(len(self.val_mlm_dataloader), len(self.val_contrastive_dataloader))
            
            for step in range(steps):
                try:
                    # è·å–éªŒè¯æ‰¹æ¬¡
                    mlm_batch = next(val_mlm_iter)
                    mlm_batch = {k: v.to(self.device) for k, v in mlm_batch.items()}
                    
                    contrastive_batch = next(val_contrastive_iter)
                    contrastive_batch = {k: v.to(self.device) for k, v in contrastive_batch.items()}
                    
                    # å‰å‘ä¼ æ’­
                    results = self.trainer.forward_batch(mlm_batch, contrastive_batch)
                    
                    val_mlm_loss += results.get('mlm_loss', 0.0).item()
                    val_contrastive_loss += results.get('contrastive_loss', 0.0).item()
                    val_joint_loss += results['joint_loss'].item()
                    total_steps += 1
                    
                except StopIteration:
                    break
                except Exception as e:
                    logger.warning(f"éªŒè¯æ­¥éª¤ {step} å‡ºé”™: {e}")
                    continue
        
        if total_steps > 0:
            return {
                'val_mlm_loss': val_mlm_loss / total_steps,
                'val_contrastive_loss': val_contrastive_loss / total_steps,
                'val_joint_loss': val_joint_loss / total_steps
            }
        else:
            return {}
    
    def update_beta_ft(self, epoch: int, total_epochs: int):
        """åŠ¨æ€æ›´æ–°beta_ft"""
        if not self.use_dynamic_beta:
            return
        
        # ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒæ•´beta_ft
        # è®­ç»ƒåˆæœŸåé‡MLMï¼ŒåæœŸåé‡å¯¹æ¯”å­¦ä¹ 
        progress = epoch / total_epochs
        self.beta_ft = self.initial_beta_ft * (1 + np.cos(np.pi * progress)) / 2
        
        # æ›´æ–°è®­ç»ƒå™¨çš„beta_ft
        self.trainer.beta_ft = self.beta_ft
        
        logger.info(f"ğŸ”„ åŠ¨æ€beta_ftæ›´æ–°: {self.beta_ft:.3f} (epoch {epoch}/{total_epochs})")
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        try:
            self.model.train()
            
            # å¦‚æœä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œè®¾ç½®epochç”¨äºshuffling
            if self.use_distributed:
                if self.mlm_sampler is not None:
                    self.mlm_sampler.set_epoch(epoch)
                if self.contrastive_sampler is not None:
                    self.contrastive_sampler.set_epoch(epoch)
            
            # åˆ›å»ºæ•°æ®è¿­ä»£å™¨
            logger.info(f"åˆ›å»ºæ•°æ®è¿­ä»£å™¨...")
            mlm_iter = iter(self.mlm_dataloader)
            contrastive_iter = iter(self.contrastive_dataloader)
            logger.info(f"æ•°æ®è¿­ä»£å™¨åˆ›å»ºæˆåŠŸ")
            
            # è®¡ç®—æ€»æ­¥æ•°ï¼ˆå–ä¸¤ä¸ªæ•°æ®é›†çš„æœ€å°å€¼ï¼‰
            total_steps = min(len(self.mlm_dataloader), len(self.contrastive_dataloader))
            
            epoch_mlm_loss = 0.0
            epoch_contrastive_loss = 0.0
            epoch_joint_loss = 0.0
            
            logger.info(f"å¼€å§‹è®­ç»ƒå¾ªç¯ï¼Œæ€»æ­¥æ•°: {total_steps}")
            progress_bar = tqdm(range(total_steps), desc=f"Epoch {epoch}")
            
            for step in progress_bar:
                try:
                    self.optimizer.zero_grad()
                    
                    # è·å–MLMæ‰¹æ¬¡
                    try:
                        mlm_batch = next(mlm_iter)
                        mlm_batch = {k: v.to(self.device) for k, v in mlm_batch.items()}
                    except StopIteration:
                        mlm_iter = iter(self.mlm_dataloader)
                        mlm_batch = next(mlm_iter)
                        mlm_batch = {k: v.to(self.device) for k, v in mlm_batch.items()}
                    
                    # è·å–å¯¹æ¯”å­¦ä¹ æ‰¹æ¬¡
                    try:
                        contrastive_batch = next(contrastive_iter)
                        contrastive_batch = {k: v.to(self.device) for k, v in contrastive_batch.items()}
                    except StopIteration:
                        contrastive_iter = iter(self.contrastive_dataloader)
                        contrastive_batch = next(contrastive_iter)
                        contrastive_batch = {k: v.to(self.device) for k, v in contrastive_batch.items()}
                    
                    # å‰å‘ä¼ æ’­
                    results = self.trainer.forward_batch(mlm_batch, contrastive_batch)
                    
                    # åå‘ä¼ æ’­
                    joint_loss = results['joint_loss']
                    joint_loss.backward()
                    
                    # è¿½è¸ªæ¢¯åº¦èŒƒæ•°
                    self.grad_tracker.update(self.model)
                    
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    
                    # ä¼˜åŒ–æ­¥éª¤
                    self.optimizer.step()
                    if self.scheduler:
                        self.scheduler.step()
                    
                    # ç´¯è®¡æŸå¤±
                    epoch_mlm_loss += results.get('mlm_loss', 0.0).item()
                    epoch_contrastive_loss += results.get('contrastive_loss', 0.0).item()
                    epoch_joint_loss += joint_loss.item()
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    progress_bar.set_postfix({
                        'MLM_Loss': f"{results.get('mlm_loss', 0.0).item():.4f}",
                        'Cont_Loss': f"{results.get('contrastive_loss', 0.0).item():.4f}",
                        'Joint_Loss': f"{joint_loss.item():.4f}",
                        'LR': f"{self.optimizer.param_groups[0]['lr']:.6f}"
                    })
                    
                except Exception as e:
                    logger.error(f"è®­ç»ƒæ­¥éª¤ {step} å‡ºé”™: {e}")
                    if self.use_distributed:
                        # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œéœ€è¦æ¸…ç†è¿›ç¨‹ç»„
                        import torch.distributed as dist
                        if dist.is_initialized():
                            dist.destroy_process_group()
                    raise e
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_mlm_loss = epoch_mlm_loss / total_steps if total_steps > 0 else 0.0
            avg_contrastive_loss = epoch_contrastive_loss / total_steps if total_steps > 0 else 0.0
            avg_joint_loss = epoch_joint_loss / total_steps if total_steps > 0 else 0.0
            
            return {
                'mlm_loss': avg_mlm_loss,
                'contrastive_loss': avg_contrastive_loss,
                'joint_loss': avg_joint_loss
            }
            
        except Exception as e:
            logger.error(f"è®­ç»ƒepoch {epoch} å¤±è´¥: {e}")
            if self.use_distributed:
                # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œéœ€è¦æ¸…ç†è¿›ç¨‹ç»„
                import torch.distributed as dist
                if dist.is_initialized():
                    dist.destroy_process_group()
            raise e
    
    def train(self, num_epochs: int, save_dir: str):
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
        logger.info(f"å¼€å§‹è”åˆè®­ç»ƒ - {num_epochs} epochs")
        logger.info(f"Î²_ft = {self.beta_ft} (MLMæƒé‡: {self.beta_ft:.2f}, å¯¹æ¯”å­¦ä¹ æƒé‡: {1-self.beta_ft:.2f})")
        
        if self.use_dynamic_beta:
            logger.info("ğŸ”„ å¯ç”¨åŠ¨æ€beta_ftè°ƒæ•´")
        if self.use_dynamic_lr:
            logger.info("ğŸ“ˆ å¯ç”¨åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´")
        if self.early_stopping:
            logger.info(f"ğŸ›‘ å¯ç”¨æ—©åœæœºåˆ¶ (patience={self.early_stopping.patience})")
        
        os.makedirs(save_dir, exist_ok=True)
        
        best_joint_loss = float('inf')
        best_val_loss = float('inf')
        start_time = time.time()
        
        for epoch in range(1, num_epochs + 1):
            logger.info(f"\n=== Epoch {epoch}/{num_epochs} ===")
            
            # åŠ¨æ€æ›´æ–°beta_ft
            self.update_beta_ft(epoch, num_epochs)
            
            # è®­ç»ƒä¸€ä¸ªepoch
            epoch_results = self.train_epoch(epoch)
            
            # éªŒè¯
            val_results = self.validate()
            
            # è®°å½•å½“å‰å­¦ä¹ ç‡
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # è·å–æ¢¯åº¦ç»Ÿè®¡
            grad_stats = self.grad_tracker.get_stats()
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            combined_results = {
                'epoch': epoch,
                'learning_rate': current_lr,
                'beta_ft': self.beta_ft,
                **epoch_results,
                **val_results,
                **grad_stats
            }
            
            self.training_stats['epoch_losses'].append(combined_results)
            self.training_stats['mlm_losses'].append(epoch_results['mlm_loss'])
            self.training_stats['contrastive_losses'].append(epoch_results['contrastive_loss'])
            self.training_stats['joint_losses'].append(epoch_results['joint_loss'])
            self.training_stats['learning_rates'].append(current_lr)
            self.training_stats['beta_ft_history'].append(self.beta_ft)
            
            if val_results:
                self.training_stats['val_losses'].append(val_results['val_joint_loss'])
            if grad_stats:
                self.training_stats['gradient_stats'].append(grad_stats)
            
            # æ‰“å°ç»“æœ
            logger.info(f"Epoch {epoch} Results:")
            logger.info(f"  è®­ç»ƒ - MLM: {epoch_results['mlm_loss']:.4f}, "
                       f"å¯¹æ¯”å­¦ä¹ : {epoch_results['contrastive_loss']:.4f}, "
                       f"è”åˆ: {epoch_results['joint_loss']:.4f}")
            
            if val_results:
                logger.info(f"  éªŒè¯ - MLM: {val_results['val_mlm_loss']:.4f}, "
                           f"å¯¹æ¯”å­¦ä¹ : {val_results['val_contrastive_loss']:.4f}, "
                           f"è”åˆ: {val_results['val_joint_loss']:.4f}")
            
            logger.info(f"  å­¦ä¹ ç‡: {current_lr:.6f}, Î²_ft: {self.beta_ft:.3f}")
            
            if grad_stats:
                logger.info(f"  æ¢¯åº¦èŒƒæ•°: å¹³å‡={grad_stats['mean_grad_norm']:.4f}, "
                           f"æœ€å¤§={grad_stats['max_grad_norm']:.4f}")
            
            # åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´
            if self.use_dynamic_lr and val_results:
                self.lr_scheduler.step(val_results['val_joint_loss'])
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯æŸå¤±ï¼Œå¦‚æœæœ‰çš„è¯ï¼‰
            current_loss = val_results.get('val_joint_loss', epoch_results['joint_loss'])
            
            if current_loss < best_joint_loss:
                best_joint_loss = current_loss
                self.save_model(save_dir, epoch, is_best=True)
                logger.info(f"  ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! (æŸå¤±: {best_joint_loss:.4f})")
            
            # æ—©åœæ£€æŸ¥
            if self.early_stopping and val_results:
                val_loss = val_results['val_joint_loss']
                if self.early_stopping(val_loss, self.model):
                    logger.info(f"ğŸ›‘ è®­ç»ƒæå‰åœæ­¢åœ¨epoch {epoch}")
                    break
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % 5 == 0:
                self.save_model(save_dir, epoch, is_best=False)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œç»Ÿè®¡ä¿¡æ¯
        self.save_model(save_dir, epoch, is_best=False, is_final=True)
        self.save_training_stats(save_dir)
        
        # è®¡ç®—è®­ç»ƒæ—¶é—´
        training_time = time.time() - start_time
        
        logger.info(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        logger.info(f"è®­ç»ƒæ—¶é—´: {training_time/3600:.2f} å°æ—¶")
        logger.info(f"æœ€ä½³æŸå¤±: {best_joint_loss:.4f}")
        logger.info(f"æœ€ç»ˆå­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.6f}")
        logger.info(f"æ¨¡å‹ä¿å­˜åœ¨: {save_dir}")
    
    def save_model(self, save_dir: str, epoch: int, is_best: bool = False, is_final: bool = False):
        """ä¿å­˜æ¨¡å‹"""
        if is_best:
            model_path = os.path.join(save_dir, 'best_model')
        elif is_final:
            model_path = os.path.join(save_dir, 'final_model')
        else:
            model_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch}')
        
        os.makedirs(model_path, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹æƒé‡ - åŒæ—¶ä¿å­˜pytorch_model.binå’Œsafetensorsæ ¼å¼
        torch.save(self.model.state_dict(), os.path.join(model_path, 'pytorch_model.bin'))
        
        # å°è¯•ä¿å­˜safetensorsæ ¼å¼
        try:
            from safetensors.torch import save_file
            save_file(self.model.state_dict(), os.path.join(model_path, 'model.safetensors'))
            logger.info(f"âœ… å·²ä¿å­˜safetensorsæ ¼å¼: {model_path}/model.safetensors")
        except ImportError:
            logger.warning("âš ï¸ safetensorsåº“æœªå®‰è£…ï¼Œè·³è¿‡safetensorsæ ¼å¼ä¿å­˜")
        except Exception as e:
            logger.warning(f"âš ï¸ safetensorsä¿å­˜å¤±è´¥: {e}")
        
        # åˆ›å»ºå®Œæ•´çš„æ¨¡å‹é…ç½®ï¼ˆåŸºäºBERTé…ç½® + æˆ‘ä»¬çš„æ‰©å±•ï¼‰
        # å¤„ç†DistributedDataParallelåŒ…è£…çš„æƒ…å†µ
        model_to_use = self.model.module if hasattr(self.model, 'module') else self.model
        model_config = model_to_use.bert_config.to_dict()
        
        # æ·»åŠ æˆ‘ä»¬çš„è‡ªå®šä¹‰é…ç½®
        model_config.update({
            "architectures": ["JointBertModel"],
            "model_type": "joint_bert",
            "custom_config": {
                "contrastive_dim": model_to_use.contrastive_dim,
                "temperature": model_to_use.temperature,
                "beta_ft": self.beta_ft,
                "use_enhanced_tokenizer": hasattr(model_to_use, 'enhanced_tokenizer_used'),
                "vocab_size": model_to_use.bert_config.vocab_size
            }
        })
        
        # ä¿å­˜å®Œæ•´çš„æ¨¡å‹é…ç½®
        with open(os.path.join(model_path, 'config.json'), 'w', encoding='utf-8') as f:
            json.dump(model_config, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è®­ç»ƒç‰¹å®šé…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
        training_config = {
            'epoch': epoch,
            'beta_ft': self.beta_ft,
            'contrastive_dim': model_to_use.contrastive_dim,
            'temperature': model_to_use.temperature,
            'model_class': 'JointBertModel'
        }
        
        with open(os.path.join(model_path, 'training_config.json'), 'w', encoding='utf-8') as f:
            json.dump(training_config, f, indent=2, ensure_ascii=False)
        
        # å¦‚æœä½¿ç”¨äº†å¢å¼ºtokenizerï¼Œå¤åˆ¶æ‰€æœ‰tokenizeræ–‡ä»¶
        if hasattr(model_to_use, 'enhanced_tokenizer_used') and model_to_use.enhanced_tokenizer_used:
            # ä¿å­˜tokenizeré…ç½®
            tokenizer_config = {
                "model_max_length": 512,
                "tokenizer_class": "BertTokenizer",
                "enhanced_tokenizer": True,
                "enhanced_tokenizer_path": "./enhanced_communication_tokenizer"
            }
            with open(os.path.join(model_path, 'tokenizer_config.json'), 'w', encoding='utf-8') as f:
                json.dump(tokenizer_config, f, indent=2, ensure_ascii=False)
            
            # å¤åˆ¶tokenizeræ ¸å¿ƒæ–‡ä»¶
            import shutil
            enhanced_tokenizer_path = "./enhanced_communication_tokenizer"
            tokenizer_files = [
                'tokenizer.json',
                'vocab.txt', 
                'decomposition_mapping.json'
            ]
            
            for file_name in tokenizer_files:
                src_file = os.path.join(enhanced_tokenizer_path, file_name)
                dst_file = os.path.join(model_path, file_name)
                if os.path.exists(src_file):
                    shutil.copy2(src_file, dst_file)
                    logger.info(f"âœ… å·²å¤åˆ¶tokenizeræ–‡ä»¶: {file_name}")
                else:
                    logger.warning(f"âš ï¸ tokenizeræ–‡ä»¶ä¸å­˜åœ¨: {src_file}")
            
            # ä¿å­˜è¯æ±‡è¡¨ä¿¡æ¯
            vocab_info = {
                "original_vocab_size": 30522,
                "extended_vocab_size": 30680,
                "added_tokens": 158,
                "source": "enhanced_communication_tokenizer"
            }
            with open(os.path.join(model_path, 'vocab_info.json'), 'w', encoding='utf-8') as f:
                json.dump(vocab_info, f, indent=2, ensure_ascii=False)
    
    def save_training_stats(self, save_dir: str):
        """ä¿å­˜è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        stats_path = os.path.join(save_dir, 'training_stats.json')
        with open(stats_path, 'w') as f:
            json.dump(self.training_stats, f, indent=2)
        
        # ä¿å­˜CSVæ ¼å¼çš„æŸå¤±æ•°æ®
        df = pd.DataFrame(self.training_stats['epoch_losses'])
        df.to_csv(os.path.join(save_dir, 'training_losses.csv'), index=False)


def main():
    parser = argparse.ArgumentParser(description="è”åˆMLM+å¯¹æ¯”å­¦ä¹ BERTæ¨¡å‹è®­ç»ƒ")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, 
                       default="./paper_spec_training_data",
                       help="è®­ç»ƒæ•°æ®ç›®å½•")
    parser.add_argument("--mlm_data_file", type=str, 
                       default="paper_spec_mlm_data.csv",
                       help="MLMè®­ç»ƒæ•°æ®æ–‡ä»¶å")
    parser.add_argument("--contrastive_data_file", type=str,
                       default="paper_spec_contrastive_data.csv",
                       help="å¯¹æ¯”å­¦ä¹ è®­ç»ƒæ•°æ®æ–‡ä»¶å")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--bert_model_path", type=str,
                       default="./bert-base-uncased",
                       help="BERTæ¨¡å‹è·¯å¾„")
    parser.add_argument("--use_enhanced_tokenizer", action="store_true",
                       help="ä½¿ç”¨å¢å¼ºé€šä¿¡tokenizer")
    parser.add_argument("--no_enhanced_tokenizer", action="store_true",
                       help="ç¦ç”¨å¢å¼ºtokenizerï¼Œå¼ºåˆ¶ä½¿ç”¨æ ‡å‡†BERT tokenizerï¼ˆé»˜è®¤ä½¿ç”¨å¢å¼ºtokenizerï¼‰")
    parser.add_argument("--enhanced_tokenizer_path", type=str,
                       default="./enhanced_communication_tokenizer",
                       help="å¢å¼ºtokenizerè·¯å¾„")
    parser.add_argument("--contrastive_dim", type=int, default=128,
                       help="å¯¹æ¯”å­¦ä¹ åµŒå…¥ç»´åº¦")
    parser.add_argument("--temperature", type=float, default=0.07,
                       help="å¯¹æ¯”å­¦ä¹ æ¸©åº¦å‚æ•°")
    parser.add_argument("--beta_ft", type=float, default=0.7,
                       help="è”åˆæŸå¤±æƒé‡å‚æ•° Î²_ft (è®ºæ–‡è§„æ ¼: 0.7)")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=64,
                       help="æ‰¹æ¬¡å¤§å°ï¼ˆæ¯ä¸ªGPUçš„æ‰¹æ¬¡å¤§å°ï¼‰")
    parser.add_argument("--num_epochs", type=int, default=30,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=2e-5,
                       help="å­¦ä¹ ç‡")
    parser.add_argument("--warmup_ratio", type=float, default=0.1,
                       help="é¢„çƒ­æ­¥æ•°æ¯”ä¾‹")
    parser.add_argument("--max_length", type=int, default=128,
                       help="æœ€å¤§åºåˆ—é•¿åº¦")
    
    # ä¼˜åŒ–å‚æ•°
    parser.add_argument("--use_validation", action="store_true",
                       help="ä½¿ç”¨éªŒè¯é›†è¯„ä¼°")
    parser.add_argument("--validation_split", type=float, default=0.2,
                       help="éªŒè¯é›†æ¯”ä¾‹")
    parser.add_argument("--early_stopping", action="store_true",
                       help="å¯ç”¨æ—©åœæœºåˆ¶")
    parser.add_argument("--patience", type=int, default=7,
                       help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--min_delta", type=float, default=0.001,
                       help="æ—©åœæœ€å°æ”¹å–„é˜ˆå€¼")
    parser.add_argument("--use_dynamic_lr", action="store_true",
                       help="ä½¿ç”¨åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´")
    parser.add_argument("--use_dynamic_beta", action="store_true",
                       help="ä½¿ç”¨åŠ¨æ€beta_ftè°ƒæ•´")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output_dir", type=str,
                       default="./final_30_epoch_output",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­")
    
    # GPUå’Œåˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
    parser.add_argument("--use_multi_gpu", action="store_true",
                       help="ä½¿ç”¨å¤šGPUè®­ç»ƒ")
    parser.add_argument("--gpu_ids", type=str, default=None,
                       help="æŒ‡å®šGPU IDï¼Œæ ¼å¼: 0,1,2 (Noneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU)")
    parser.add_argument("--dataloader_num_workers", type=int, default=4,
                       help="æ•°æ®åŠ è½½å™¨å·¥ä½œçº¿ç¨‹æ•°")
    parser.add_argument("--no_pin_memory", action="store_true",
                       help="ç¦ç”¨å†…å­˜å›ºå®šï¼ˆé»˜è®¤å¯ç”¨å†…å­˜å›ºå®šä»¥åŠ é€Ÿæ•°æ®ä¼ è¾“ï¼‰")
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # å¤„ç†pin_memoryå‚æ•° - é»˜è®¤å¯ç”¨ï¼Œé™¤éæŒ‡å®š--no_pin_memory
    pin_memory = not args.no_pin_memory if hasattr(args, 'no_pin_memory') else True
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
    if args.use_multi_gpu and torch.cuda.is_available():
        # æ£€æŸ¥æ˜¯å¦åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­
        if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
            # åˆ†å¸ƒå¼è®­ç»ƒ
            local_rank = int(os.environ.get("LOCAL_RANK", 0))
            rank = int(os.environ.get("RANK", 0))
            world_size = int(os.environ.get("WORLD_SIZE", 1))
            
            # åˆå§‹åŒ–è¿›ç¨‹ç»„
            dist.init_process_group(backend="nccl")
            torch.cuda.set_device(local_rank)
            device = torch.device(f"cuda:{local_rank}")
            
            logger.info(f"ğŸš€ åˆ†å¸ƒå¼è®­ç»ƒ - Rank: {rank}/{world_size}, Local Rank: {local_rank}")
        else:
            # å¤šGPUä½†éåˆ†å¸ƒå¼è®­ç»ƒï¼ˆDataParallelï¼‰
            device = torch.device("cuda")
            logger.info(f"ğŸš€ å¤šGPUè®­ç»ƒ (DataParallel) - {torch.cuda.device_count()} GPUs")
    else:
        # å•GPUæˆ–CPUè®­ç»ƒ
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ğŸš€ å•è®¾å¤‡è®­ç»ƒ: {device}")
    
    # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°é…ç½®ä¿¡æ¯
    if not args.use_multi_gpu or not ('RANK' in os.environ) or int(os.environ.get("RANK", 0)) == 0:
        logger.info("ğŸš€ å¼€å§‹è”åˆBERTæ¨¡å‹è®­ç»ƒ...")
        logger.info(f"ğŸ“‹ è®­ç»ƒé…ç½®:")
        logger.info(f"  - æ•°æ®ç›®å½•: {args.data_dir}")
        logger.info(f"  - BERTæ¨¡å‹: {args.bert_model_path}")
        logger.info(f"  - æ‰¹æ¬¡å¤§å°: {args.batch_size}")
        logger.info(f"  - è®­ç»ƒè½®æ•°: {args.num_epochs}")
        logger.info(f"  - å­¦ä¹ ç‡: {args.learning_rate}")
        logger.info(f"  - Î²_ft: {args.beta_ft}")
        logger.info(f"  - å¯¹æ¯”å­¦ä¹ ç»´åº¦: {args.contrastive_dim}")
        logger.info(f"  - æ¸©åº¦å‚æ•°: {args.temperature}")
        logger.info(f"  - å¤šGPUè®­ç»ƒ: {args.use_multi_gpu}")
        logger.info(f"  - æ•°æ®åŠ è½½å·¥ä½œçº¿ç¨‹: {args.dataloader_num_workers}")
        logger.info(f"  - å†…å­˜å›ºå®š: {pin_memory}")
    
    # åŠ è½½åˆ†è¯å™¨ - é»˜è®¤ä½¿ç”¨å¢å¼ºtokenizerï¼Œé™¤éæ˜ç¡®æŒ‡å®š--no_enhanced_tokenizer
    if hasattr(args, 'no_enhanced_tokenizer') and args.no_enhanced_tokenizer:
        logger.info(f"ğŸ“š ç”¨æˆ·æŒ‡å®šä½¿ç”¨æ ‡å‡†tokenizer: {args.bert_model_path}")
        tokenizer = AutoTokenizer.from_pretrained(args.bert_model_path)
        use_enhanced_tokenizer = False
    else:
        logger.info("ğŸš€ ä½¿ç”¨å¢å¼ºé€šä¿¡tokenizerï¼ˆæ”¯æŒè‡ªåŠ¨åˆ›å»ºï¼‰")
        tokenizer = get_enhanced_tokenizer(auto_create=True)
        use_enhanced_tokenizer = True
    
    # åˆ›å»ºæ•°æ®é›†
    mlm_data_path = os.path.join(args.data_dir, args.mlm_data_file)
    contrastive_data_path = os.path.join(args.data_dir, args.contrastive_data_file)
    
    logger.info("ğŸ“Š åŠ è½½æ•°æ®é›†...")
    full_mlm_dataset = MLMDataset(mlm_data_path, tokenizer, args.max_length)
    full_contrastive_dataset = ContrastiveDataset(contrastive_data_path, tokenizer, args.max_length)
    
    # åˆ†å‰²æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    val_mlm_dataloader = None
    val_contrastive_dataloader = None
    
    if args.use_validation:
        # è®¡ç®—åˆ†å‰²å¤§å°
        mlm_val_size = int(len(full_mlm_dataset) * args.validation_split)
        mlm_train_size = len(full_mlm_dataset) - mlm_val_size
        
        contrastive_val_size = int(len(full_contrastive_dataset) * args.validation_split)
        contrastive_train_size = len(full_contrastive_dataset) - contrastive_val_size
        
        # åˆ†å‰²MLMæ•°æ®é›†
        mlm_dataset, val_mlm_dataset = random_split(
            full_mlm_dataset, 
            [mlm_train_size, mlm_val_size],
            generator=torch.Generator().manual_seed(args.seed)
        )
        
        # åˆ†å‰²å¯¹æ¯”å­¦ä¹ æ•°æ®é›†
        contrastive_dataset, val_contrastive_dataset = random_split(
            full_contrastive_dataset, 
            [contrastive_train_size, contrastive_val_size],
            generator=torch.Generator().manual_seed(args.seed)
        )
        
        logger.info(f"ğŸ“Š æ•°æ®é›†åˆ†å‰²:")
        logger.info(f"  MLM - è®­ç»ƒ: {len(mlm_dataset)}, éªŒè¯: {len(val_mlm_dataset)}")
        logger.info(f"  å¯¹æ¯”å­¦ä¹  - è®­ç»ƒ: {len(contrastive_dataset)}, éªŒè¯: {len(val_contrastive_dataset)}")
        
        # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
        val_mlm_dataloader = DataLoader(
            val_mlm_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=args.dataloader_num_workers,
            pin_memory=pin_memory,
            collate_fn=collate_mlm_batch
        )
        
        val_contrastive_dataloader = DataLoader(
            val_contrastive_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=args.dataloader_num_workers,
            pin_memory=pin_memory,
            collate_fn=collate_contrastive_batch
        )
    else:
        mlm_dataset = full_mlm_dataset
        contrastive_dataset = full_contrastive_dataset
        logger.info("ğŸ“Š ä½¿ç”¨å…¨é‡æ•°æ®é›†è®­ç»ƒï¼ˆæ— éªŒè¯é›†ï¼‰")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼é‡‡æ ·
    use_distributed_sampler = args.use_multi_gpu and 'RANK' in os.environ
    
    if use_distributed_sampler:
        mlm_sampler = DistributedSampler(mlm_dataset)
        contrastive_sampler = DistributedSampler(contrastive_dataset)
        shuffle_mlm = False
        shuffle_contrastive = False
    else:
        mlm_sampler = None
        contrastive_sampler = None
        shuffle_mlm = True
        shuffle_contrastive = True
    
    mlm_dataloader = DataLoader(
        mlm_dataset, 
        batch_size=args.batch_size, 
        shuffle=shuffle_mlm,
        sampler=mlm_sampler,
        num_workers=args.dataloader_num_workers,
        pin_memory=pin_memory,
        collate_fn=collate_mlm_batch
    )
    
    contrastive_dataloader = DataLoader(
        contrastive_dataset,
        batch_size=args.batch_size,
        shuffle=shuffle_contrastive,
        sampler=contrastive_sampler,
        num_workers=args.dataloader_num_workers,
        pin_memory=pin_memory,
        collate_fn=collate_contrastive_batch
    )
    
    logger.info(f"  - MLMæ ·æœ¬: {len(mlm_dataset)}")
    logger.info(f"  - å¯¹æ¯”å­¦ä¹ æ ·æœ¬: {len(contrastive_dataset)}")
    logger.info(f"  - MLMæ‰¹æ¬¡æ•°: {len(mlm_dataloader)}")
    logger.info(f"  - å¯¹æ¯”å­¦ä¹ æ‰¹æ¬¡æ•°: {len(contrastive_dataloader)}")
    
    # åˆ›å»ºæ¨¡å‹
    logger.info("ğŸ¤– åˆ›å»ºè”åˆBERTæ¨¡å‹...")
    model = JointBertModel(
        bert_model_path=args.bert_model_path,
        contrastive_dim=args.contrastive_dim,
        temperature=args.temperature,
        use_enhanced_tokenizer=use_enhanced_tokenizer
    )
    # æ ‡è®°æ˜¯å¦ä½¿ç”¨äº†å¢å¼ºtokenizer
    model.enhanced_tokenizer_used = use_enhanced_tokenizer
    model.to(device)
    
    # è®¾ç½®å¤šGPUè®­ç»ƒ
    if args.use_multi_gpu and torch.cuda.is_available():
        if 'RANK' in os.environ:
            # åˆ†å¸ƒå¼è®­ç»ƒ
            model = DDP(model, device_ids=[device.index] if device.type == 'cuda' else None,
                       find_unused_parameters=True)
            logger.info("ğŸš€ æ¨¡å‹å·²åŒ…è£…ä¸ºDistributedDataParallel")
        elif torch.cuda.device_count() > 1:
            # DataParallelè®­ç»ƒ
            model = nn.DataParallel(model)
            logger.info(f"ğŸš€ æ¨¡å‹å·²åŒ…è£…ä¸ºDataParallelï¼Œä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPU")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = AdamW(model.parameters(), lr=args.learning_rate)
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    total_steps = min(len(mlm_dataloader), len(contrastive_dataloader)) * args.num_epochs
    warmup_steps = int(total_steps * args.warmup_ratio)
    
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )
    
    logger.info(f"ğŸ“ˆ ä¼˜åŒ–å™¨é…ç½®:")
    logger.info(f"  - æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")
    logger.info(f"  - é¢„çƒ­æ­¥æ•°: {warmup_steps}")
    
    # åˆ›å»ºæ—©åœæœºåˆ¶
    early_stopping = None
    if args.early_stopping:
        early_stopping = EarlyStopping(
            patience=args.patience,
            min_delta=args.min_delta,
            restore_best_weights=True
        )
        logger.info(f"ğŸ›‘ æ—©åœé…ç½®: patience={args.patience}, min_delta={args.min_delta}")
    
    # åˆ›å»ºè®­ç»ƒç®¡ç†å™¨
    training_manager = JointTrainingManager(
        model=model,
        mlm_dataloader=mlm_dataloader,
        contrastive_dataloader=contrastive_dataloader,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        beta_ft=args.beta_ft,
        use_distributed=use_distributed_sampler,
        mlm_sampler=mlm_sampler,
        contrastive_sampler=contrastive_sampler,
        val_mlm_dataloader=val_mlm_dataloader,
        val_contrastive_dataloader=val_contrastive_dataloader,
        early_stopping=early_stopping,
        use_dynamic_lr=args.use_dynamic_lr,
        use_dynamic_beta=args.use_dynamic_beta
    )
    
    # å¼€å§‹è®­ç»ƒ
    training_manager.train(args.num_epochs, args.output_dir)
    
    logger.info("ğŸ‰ è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()
