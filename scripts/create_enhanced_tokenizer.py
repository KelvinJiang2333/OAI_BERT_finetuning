#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ›å»ºå¢å¼ºé€šä¿¡tokenizer

åŸºäºBERT-base-uncasedæ‰©å±•é€šä¿¡é¢†åŸŸä¸“ä¸šæœ¯è¯­
"""

import os
import json
import shutil
import logging
from typing import List, Dict, Set
from transformers import AutoTokenizer, BertTokenizer
import torch

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedTokenizerBuilder:
    """å¢å¼ºtokenizeræ„å»ºå™¨"""
    
    def __init__(self, base_tokenizer_path: str):
        self.base_tokenizer_path = base_tokenizer_path
        self.base_tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_path)
        
        # é€šä¿¡é¢†åŸŸä¸“ä¸šæœ¯è¯­åˆ—è¡¨
        self.communication_terms = [
            # 5G/NRæœ¯è¯­
            "timing_advance", "harq_processes", "aggregation_level",
            "scheduling_offset", "dci_format", "pdcch_config",
            "pucch_config", "srs_config", "csi_rs_config",
            "prach_config", "rach_config", "rnti_config",
            
            # LTEæœ¯è¯­  
            "enb_config", "cell_config", "ue_context",
            "radio_bearer", "eps_bearer", "qos_config",
            "measurement_config", "handover_config",
            
            # è°ƒåº¦ç›¸å…³
            "resource_allocation", "modulation_coding",
            "transport_block", "code_block", "rate_matching",
            "channel_coding", "scrambling_sequence",
            
            # ç‰©ç†å±‚
            "subcarrier_spacing", "cyclic_prefix", "guard_interval",
            "ofdm_symbol", "resource_element", "resource_block",
            "bandwidth_part", "carrier_frequency",
            
            # MACå±‚
            "mac_header", "mac_payload", "buffer_status",
            "scheduling_request", "random_access", "contention_resolution",
            
            # RLCå±‚
            "rlc_header", "sequence_number", "acknowledged_mode",
            "unacknowledged_mode", "transparent_mode",
            
            # PDCPå±‚
            "pdcp_header", "compression_config", "integrity_protection",
            "ciphering_config", "rohc_config",
            
            # RRCå±‚
            "rrc_connection", "system_information", "measurement_report",
            "capability_information", "security_config",
            
            # ç½‘ç»œæ¶æ„
            "core_network", "access_network", "transport_network",
            "network_function", "service_function",
            
            # åè®®æ ˆ
            "protocol_stack", "layer_interface", "service_access_point",
            "protocol_data_unit", "service_data_unit",
            
            # ä¿¡ä»¤æµç¨‹
            "attach_procedure", "detach_procedure", "registration_procedure",
            "authentication_procedure", "authorization_procedure",
            
            # QoSç›¸å…³
            "quality_of_service", "service_level_agreement",
            "traffic_shaping", "congestion_control",
            
            # ç§»åŠ¨æ€§ç®¡ç†
            "mobility_management", "location_management",
            "handover_management", "load_balancing",
            
            # å®‰å…¨ç›¸å…³
            "authentication_vector", "security_algorithm",
            "encryption_key", "integrity_key", "security_context",
            
            # æ— çº¿èµ„æºç®¡ç†
            "radio_resource_management", "interference_coordination",
            "power_control", "admission_control",
            
            # æ€§èƒ½ä¼˜åŒ–
            "throughput_optimization", "latency_optimization", 
            "energy_efficiency", "spectral_efficiency",
            
            # æµ‹è¯•å’Œæµ‹é‡
            "performance_monitoring", "key_performance_indicator",
            "network_optimization", "fault_management",
            
            # æ–°å¢æœ¯è¯­
            "beam_management", "massive_mimo", "network_slicing",
            "edge_computing", "ultra_reliable", "low_latency",
            "enhanced_mobile", "machine_type", "critical_communication",
            
            # ç®—æ³•ç›¸å…³
            "scheduling_algorithm", "resource_allocation_algorithm",
            "channel_estimation", "signal_processing",
            "error_correction", "adaptive_modulation",
            
            # æ•°æ®ç»“æ„
            "configuration_parameters", "measurement_parameters",
            "status_indication", "event_notification",
            "timer_configuration", "counter_management",
            
            # å‡½æ•°å’Œè¿‡ç¨‹
            "initialization_procedure", "configuration_procedure",
            "monitoring_procedure", "cleanup_procedure",
            "error_handling", "exception_processing",
            
            # ç‰¹å®šå®ç°
            "gnb_scheduler", "enb_scheduler", "mac_scheduler",
            "rlc_entity", "pdcp_entity", "rrc_entity",
            "nas_entity", "mm_entity", "sm_entity"
        ]
        
    def build_enhanced_tokenizer(self, output_dir: str) -> bool:
        """æ„å»ºå¢å¼ºtokenizer"""
        try:
            logger.info("ğŸš€ å¼€å§‹æ„å»ºå¢å¼ºé€šä¿¡tokenizer...")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(output_dir, exist_ok=True)
            
            # 1. å¤åˆ¶åŸºç¡€tokenizeræ–‡ä»¶
            self._copy_base_tokenizer_files(output_dir)
            
            # 2. æ‰©å±•è¯æ±‡è¡¨
            original_vocab_size = self._extend_vocabulary(output_dir)
            
            # 3. åˆ›å»ºæ‰©å±•åµŒå…¥çŸ©é˜µ
            self._create_extended_embeddings(output_dir, original_vocab_size)
            
            # 4. åˆ›å»ºæœ¯è¯­åˆ†è§£æ˜ å°„
            self._create_decomposition_mapping(output_dir)
            
            # 5. æ›´æ–°é…ç½®æ–‡ä»¶
            self._update_tokenizer_config(output_dir)
            
            # 6. éªŒè¯tokenizer
            self._validate_tokenizer(output_dir)
            
            logger.info(f"âœ… å¢å¼ºtokenizeræ„å»ºå®Œæˆ: {output_dir}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºå¢å¼ºtokenizerå¤±è´¥: {e}")
            return False
    
    def _copy_base_tokenizer_files(self, output_dir: str):
        """å¤åˆ¶åŸºç¡€tokenizeræ–‡ä»¶"""
        logger.info("ğŸ“ å¤åˆ¶åŸºç¡€tokenizeræ–‡ä»¶...")
        
        base_files = ['tokenizer.json', 'tokenizer_config.json', 'vocab.txt']
        
        for file_name in base_files:
            src_path = os.path.join(self.base_tokenizer_path, file_name)
            dst_path = os.path.join(output_dir, file_name)
            
            if os.path.exists(src_path):
                shutil.copy2(src_path, dst_path)
                logger.info(f"  âœ“ å¤åˆ¶: {file_name}")
            else:
                logger.warning(f"  âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {src_path}")
    
    def _extend_vocabulary(self, output_dir: str) -> int:
        """æ‰©å±•è¯æ±‡è¡¨"""
        logger.info("ğŸ“š æ‰©å±•è¯æ±‡è¡¨...")
        
        vocab_path = os.path.join(output_dir, 'vocab.txt')
        
        # è¯»å–åŸå§‹è¯æ±‡è¡¨
        with open(vocab_path, 'r', encoding='utf-8') as f:
            original_vocab = [line.strip() for line in f]
        
        original_size = len(original_vocab)
        logger.info(f"  åŸå§‹è¯æ±‡è¡¨å¤§å°: {original_size}")
        
        # è·å–ç°æœ‰è¯æ±‡é›†åˆ
        existing_vocab = set(original_vocab)
        
        # æ·»åŠ æ–°æœ¯è¯­ï¼ˆæ£€æŸ¥é‡å¤ï¼‰
        new_terms = []
        for term in self.communication_terms:
            if term not in existing_vocab:
                new_terms.append(term)
                existing_vocab.add(term)
        
        # å†™å…¥æ‰©å±•åçš„è¯æ±‡è¡¨
        extended_vocab = original_vocab + new_terms
        
        with open(vocab_path, 'w', encoding='utf-8') as f:
            for word in extended_vocab:
                f.write(f"{word}\n")
        
        logger.info(f"  æ–°å¢æœ¯è¯­: {len(new_terms)} ä¸ª")
        logger.info(f"  æ‰©å±•åè¯æ±‡è¡¨å¤§å°: {len(extended_vocab)}")
        
        return original_size
    
    def _create_extended_embeddings(self, output_dir: str, original_vocab_size: int):
        """åˆ›å»ºæ‰©å±•åµŒå…¥çŸ©é˜µ"""
        logger.info("ğŸ§  åˆ›å»ºæ‰©å±•åµŒå…¥çŸ©é˜µ...")
        
        # åŠ è½½åŸå§‹æ¨¡å‹ä»¥è·å–åµŒå…¥çŸ©é˜µ
        try:
            from transformers import BertModel
            bert_model = BertModel.from_pretrained(self.base_tokenizer_path)
            original_embeddings = bert_model.embeddings.word_embeddings.weight.data
            
            embedding_dim = original_embeddings.shape[1]
            new_vocab_size = original_vocab_size + len(self.communication_terms)
            
            # åˆ›å»ºæ‰©å±•åµŒå…¥çŸ©é˜µ
            extended_embeddings = torch.zeros(new_vocab_size, embedding_dim)
            
            # å¤åˆ¶åŸå§‹åµŒå…¥
            extended_embeddings[:original_vocab_size] = original_embeddings[:original_vocab_size]
            
            # ä¸ºæ–°æœ¯è¯­åˆå§‹åŒ–åµŒå…¥ï¼ˆä½¿ç”¨æ­£æ€åˆ†å¸ƒï¼‰
            std = original_embeddings.std().item()
            extended_embeddings[original_vocab_size:] = torch.normal(
                mean=0.0, std=std, size=(len(self.communication_terms), embedding_dim)
            )
            
            # ä¿å­˜æ‰©å±•åµŒå…¥
            embedding_path = os.path.join(output_dir, 'extended_embeddings_v2.pt')
            torch.save(extended_embeddings, embedding_path)
            
            logger.info(f"  âœ“ åµŒå…¥çŸ©é˜µ: {extended_embeddings.shape}")
            logger.info(f"  âœ“ ä¿å­˜è·¯å¾„: {embedding_path}")
            
        except Exception as e:
            logger.error(f"  âŒ åˆ›å»ºåµŒå…¥çŸ©é˜µå¤±è´¥: {e}")
    
    def _create_decomposition_mapping(self, output_dir: str):
        """åˆ›å»ºæœ¯è¯­åˆ†è§£æ˜ å°„"""
        logger.info("ğŸ“ åˆ›å»ºæœ¯è¯­åˆ†è§£æ˜ å°„...")
        
        # åˆ›å»ºåˆ†è§£æ˜ å°„
        decomposition_mapping = {}
        
        for term in self.communication_terms:
            # ä½¿ç”¨åŸå§‹tokenizeråˆ†è§£æœ¯è¯­
            tokens = self.base_tokenizer.tokenize(term)
            
            decomposition_mapping[term] = {
                "original_tokens": tokens,
                "token_count": len(tokens),
                "improved_tokenization": [term],  # å¢å¼ºåç”¨å•ä¸ªtoken
                "improvement_ratio": len(tokens)  # æ”¹è¿›æ¯”ä¾‹
            }
        
        # ä¿å­˜æ˜ å°„
        mapping_path = os.path.join(output_dir, 'decomposition_mapping.json')
        with open(mapping_path, 'w', encoding='utf-8') as f:
            json.dump(decomposition_mapping, f, indent=2, ensure_ascii=False)
        
        logger.info(f"  âœ“ æœ¯è¯­æ˜ å°„: {len(decomposition_mapping)} ä¸ª")
        logger.info(f"  âœ“ ä¿å­˜è·¯å¾„: {mapping_path}")
    
    def _update_tokenizer_config(self, output_dir: str):
        """æ›´æ–°tokenizeré…ç½®"""
        logger.info("âš™ï¸ æ›´æ–°tokenizeré…ç½®...")
        
        config_path = os.path.join(output_dir, 'tokenizer_config.json')
        
        # è¯»å–åŸå§‹é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # æ›´æ–°é…ç½®
        config.update({
            "enhanced_tokenizer": True,
            "communication_domain": True,
            "added_tokens": len(self.communication_terms),
            "vocab_size": 30522 + len(self.communication_terms),
            "model_type": "enhanced_bert_communication",
            "domain": "wireless_communication",
            "version": "v2.0"
        })
        
        # ä¿å­˜æ›´æ–°åçš„é…ç½®
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        logger.info(f"  âœ“ é…ç½®æ›´æ–°å®Œæˆ")
    
    def _validate_tokenizer(self, output_dir: str):
        """éªŒè¯tokenizer"""
        logger.info("ğŸ” éªŒè¯å¢å¼ºtokenizer...")
        
        try:
            # å°è¯•åŠ è½½æ–°çš„tokenizer
            tokenizer = BertTokenizer.from_pretrained(output_dir)
            
            # æµ‹è¯•ä¸“ä¸šæœ¯è¯­åˆ†è¯
            test_terms = ["timing_advance", "harq_processes", "aggregation_level"]
            
            logger.info("  æµ‹è¯•åˆ†è¯æ•ˆæœ:")
            for term in test_terms:
                tokens = tokenizer.tokenize(term)
                logger.info(f"    {term} -> {tokens}")
            
            logger.info(f"  âœ… éªŒè¯é€šè¿‡ï¼Œè¯æ±‡è¡¨å¤§å°: {len(tokenizer.vocab)}")
            
        except Exception as e:
            logger.error(f"  âŒ éªŒè¯å¤±è´¥: {e}")

def main():
    # é…ç½®è·¯å¾„
    base_tokenizer_path = "./bert-base-uncased"
    output_dir = "./enhanced_communication_tokenizer"
    
    # æ£€æŸ¥åŸºç¡€tokenizeræ˜¯å¦å­˜åœ¨
    if not os.path.exists(base_tokenizer_path):
        logger.error(f"âŒ åŸºç¡€tokenizerä¸å­˜åœ¨: {base_tokenizer_path}")
        return
    
    # åˆ›å»ºæ„å»ºå™¨
    builder = EnhancedTokenizerBuilder(base_tokenizer_path)
    
    # æ„å»ºå¢å¼ºtokenizer
    success = builder.build_enhanced_tokenizer(output_dir)
    
    if success:
        logger.info("ğŸ‰ å¢å¼ºtokenizeråˆ›å»ºæˆåŠŸï¼")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        # æä¾›ä½¿ç”¨è¯´æ˜
        logger.info("\nğŸ“‹ ä½¿ç”¨æ–¹æ³•:")
        logger.info("python train_joint_bert.py --use_enhanced_tokenizer")
        
    else:
        logger.error("ğŸ’¥ å¢å¼ºtokenizeråˆ›å»ºå¤±è´¥")

if __name__ == "__main__":
    main()
