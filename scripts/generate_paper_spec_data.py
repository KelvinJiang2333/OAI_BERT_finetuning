#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®ºæ–‡è§„æ ¼æ•°æ®ç”Ÿæˆå™¨
æ ¹æ®è®ºæ–‡è¦æ±‚ç”Ÿæˆç²¾ç¡®çš„è®­ç»ƒæ•°æ®:
- MLM: 10,285ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªåŒ…å«3å‰+1å½“å‰+3å=7è¡Œä»£ç 
- CL: 26,609å¯¹æ ·æœ¬ï¼ŒNä¸N+1,N+2,N+3é…å¯¹
"""

import json
import re
import random
import argparse
import logging
import os
from pathlib import Path
from typing import List, Tuple, Dict, Set
from collections import defaultdict
import pandas as pd
from transformers import AutoTokenizer
from tqdm import tqdm
import numpy as np
from tokenizer_utils import get_enhanced_tokenizer

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PaperSpecDataGenerator:
    def __init__(self, tokenizer_path: str = None, seed: int = 42):
        """åˆå§‹åŒ–è®ºæ–‡è§„æ ¼æ•°æ®ç”Ÿæˆå™¨"""
        if tokenizer_path and tokenizer_path != "./enhanced_communication_tokenizer":
            logger.info(f"ğŸ”„ ä½¿ç”¨æŒ‡å®štokenizerè·¯å¾„: {tokenizer_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
        else:
            logger.info(f"ğŸš€ ä½¿ç”¨å¢å¼ºé€šä¿¡tokenizer")
            self.tokenizer = get_enhanced_tokenizer()
        
        self.seed = seed
        random.seed(seed)
        np.random.seed(seed)
        
    def generate_mlm_samples(self, file_paths: List[str], target_samples: int = 10285, 
                           context_lines: int = 3) -> List[Dict]:
        """
        ç”ŸæˆMLMæ ·æœ¬ (è®ºæ–‡è§„æ ¼)
        æ¯ä¸ªæ ·æœ¬åŒ…å«3å‰+1å½“å‰+3å=7è¡Œä»£ç ï¼Œæ©ç 15%çš„token
        """
        logger.info(f"ğŸ“ ç”Ÿæˆ{target_samples}ä¸ªMLMæ ·æœ¬ (è®ºæ–‡è§„æ ¼: {context_lines}å‰+1å½“å‰+{context_lines}å)")
        
        all_lines = []
        for file_path in file_paths:
            logger.info(f"å¤„ç†æ–‡ä»¶: {file_path}")
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = [line.strip() for line in f.readlines() if line.strip()]
                all_lines.extend([(line, file_path) for line in lines])
        
        logger.info(f"æ€»æœ‰æ•ˆè¡Œæ•°: {len(all_lines)}")
        
        mlm_samples = []
        
        # ä¸ºæ¯ä¸€è¡Œç”Ÿæˆä¸Šä¸‹æ–‡æ ·æœ¬
        for i in tqdm(range(len(all_lines)), desc="ç”ŸæˆMLMæ ·æœ¬"):
            if len(mlm_samples) >= target_samples:
                break
                
            current_line, file_path = all_lines[i]
            
            # æ”¶é›†ä¸Šä¸‹æ–‡è¡Œ (3å‰+1å½“å‰+3å)
            context_window = []
            
            # æ·»åŠ å‰é¢çš„è¡Œ
            for offset in range(-context_lines, 0):
                if i + offset >= 0:
                    prev_line, prev_file = all_lines[i + offset]
                    if prev_file == file_path:  # åŒä¸€æ–‡ä»¶
                        context_window.append(prev_line)
                    else:
                        context_window.append("")  # å ä½ç¬¦
                else:
                    context_window.append("")  # å ä½ç¬¦
            
            # æ·»åŠ å½“å‰è¡Œ
            context_window.append(current_line)
            
            # æ·»åŠ åé¢çš„è¡Œ
            for offset in range(1, context_lines + 1):
                if i + offset < len(all_lines):
                    next_line, next_file = all_lines[i + offset]
                    if next_file == file_path:  # åŒä¸€æ–‡ä»¶
                        context_window.append(next_line)
                    else:
                        context_window.append("")  # å ä½ç¬¦
                else:
                    context_window.append("")  # å ä½ç¬¦
            
            # è¿‡æ»¤ç©ºè¡Œå¹¶ç»„åˆæ–‡æœ¬
            valid_lines = [line for line in context_window if line.strip()]
            if len(valid_lines) >= 3:  # è‡³å°‘éœ€è¦3è¡Œæœ‰æ•ˆå†…å®¹
                context_text = ' '.join(valid_lines)
                
                # åº”ç”¨MLMæ©ç 
                masked_sample = self.apply_mlm_masking_single(context_text, mask_ratio=0.15)
                if masked_sample:
                    mlm_samples.append(masked_sample)
        
        logger.info(f"âœ… ç”Ÿæˆ{len(mlm_samples)}ä¸ªMLMè®­ç»ƒæ ·æœ¬")
        return mlm_samples[:target_samples]
    
    def generate_contrastive_pairs(self, file_paths: List[str], target_pairs: int = 26609) -> Tuple[List[Tuple[str, str]], List[Tuple[str, str]]]:
        """
        ç”Ÿæˆè¯è¯­çº§åˆ«å¯¹æ¯”å­¦ä¹ æ ·æœ¬å¯¹ (æ”¹è¿›ç‰ˆ)
        æ­£æ ·æœ¬: åŒä¸€è¡Œä»£ç ä¸­çš„æœ‰æ•ˆè¯è¯­é…å¯¹
        è´Ÿæ ·æœ¬: ä»£ç è¯è¯­ä¸å…¶ä»–é¢†åŸŸè¯è¯­é…å¯¹
        """
        logger.info(f"ğŸ“ ç”Ÿæˆ{target_pairs}å¯¹è¯è¯­çº§åˆ«å¯¹æ¯”å­¦ä¹ æ ·æœ¬")
        
        # æ”¶é›†æ‰€æœ‰ä»£ç è¡Œ
        all_lines = []
        for file_path in file_paths:
            logger.info(f"å¤„ç†æ–‡ä»¶: {file_path}")
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = [line.strip() for line in f.readlines() if line.strip()]
                all_lines.extend(lines)
        
        logger.info(f"æ€»æœ‰æ•ˆè¡Œæ•°: {len(all_lines)}")
        
        # ç”Ÿæˆæ­£æ ·æœ¬å¯¹: åŒä¸€è¡Œå†…çš„æœ‰æ•ˆè¯è¯­é…å¯¹
        positive_pairs = []
        
        for line in tqdm(all_lines, desc="ç”Ÿæˆè¯è¯­æ­£æ ·æœ¬å¯¹"):
            if len(positive_pairs) >= target_pairs:
                break
                
            # æå–æœ‰æ•ˆè¯è¯­
            valid_tokens = self._extract_valid_words(line)
            
            if len(valid_tokens) >= 2:
                # åœ¨åŒä¸€è¡Œå†…è¿›è¡Œè¯è¯­é…å¯¹
                for i in range(len(valid_tokens)):
                    for j in range(i + 1, len(valid_tokens)):
                        if len(positive_pairs) >= target_pairs:
                            break
                        positive_pairs.append((valid_tokens[i], valid_tokens[j]))
            if len(positive_pairs) >= target_pairs:
                break
        
        # ç”Ÿæˆè´Ÿæ ·æœ¬å¯¹: ä»£ç è¯è¯­ä¸å…¶ä»–é¢†åŸŸè¯è¯­
        negative_pairs = []
        general_words = self._generate_general_domain_words(target_pairs)
        code_words = [pair[0] for pair in positive_pairs[:target_pairs]]
        
        for i, code_word in enumerate(code_words):
            if i < len(general_words):
                negative_pairs.append((code_word, general_words[i]))
        
        logger.info(f"âœ… ç”Ÿæˆ{len(positive_pairs)}ä¸ªè¯è¯­æ­£æ ·æœ¬å¯¹ï¼Œ{len(negative_pairs)}ä¸ªè¯è¯­è´Ÿæ ·æœ¬å¯¹")
        return positive_pairs[:target_pairs], negative_pairs[:target_pairs]
    
    def _extract_valid_words(self, code_line: str) -> List[str]:
        """æå–ä»£ç è¡Œä¸­çš„å®Œæ•´æœ‰æ•ˆè¯è¯­ï¼Œåªè¿‡æ»¤å•å­—æ¯å˜é‡"""
        import re
        
        # 1. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰å¯èƒ½çš„æ ‡è¯†ç¬¦å’Œå…³é”®è¯
        # åŒ¹é…ï¼šå­—æ¯å¼€å¤´ï¼ŒåŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿çš„å®Œæ•´è¯è¯­
        word_pattern = r'\b[a-zA-Z_][a-zA-Z0-9_]*\b'
        potential_words = re.findall(word_pattern, code_line)
        
        # 2. åªè¿‡æ»¤å•å­—æ¯å˜é‡ï¼Œä¿ç•™æ‰€æœ‰å¤šå­—æ¯è¯è¯­
        single_letters = {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 
                         'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'}
        
        valid_words = []
        for word in potential_words:
            word_lower = word.lower()
            
            # ç®€å•è¿‡æ»¤æ¡ä»¶ï¼šåªæ’é™¤å•å­—æ¯
            if len(word) >= 2 or word_lower not in single_letters:
                valid_words.append(word_lower)
        
        # 3. å»é‡ä½†ä¿æŒé¡ºåº
        seen = set()
        unique_words = []
        for word in valid_words:
            if word not in seen:
                seen.add(word)
                unique_words.append(word)
        
        return unique_words
    
    def _generate_general_domain_words(self, count: int) -> List[str]:
        """ä»å¸¸ç”¨è‹±æ–‡è¯å…¸ç”Ÿæˆè´Ÿæ ·æœ¬è¯æ±‡"""
        try:
            # ä½¿ç”¨å¸¸ç”¨è¯å…¸æ–‡ä»¶
            common_words_path = os.path.join(os.path.dirname(__file__), 'google-10000-english-no-swears.txt')
            
            if os.path.exists(common_words_path):
                with open(common_words_path, 'r', encoding='utf-8') as f:
                    common_words = [line.strip().lower() for line in f if line.strip()]
                
                # è¿‡æ»¤è¯æ±‡ï¼šé•¿åº¦>=3ï¼ŒåªåŒ…å«å­—æ¯
                filtered_words = [
                    word for word in common_words 
                    if len(word) >= 3 and word.isalpha()
                ]
                
                logger.info(f"ğŸ“– ä½¿ç”¨å¸¸ç”¨è¯å…¸: {len(filtered_words)} ä¸ªå•è¯")
                
            else:
                # å¤‡ç”¨è¯åº“
                filtered_words = [
                    "weather", "sunny", "rain", "cloud", "wind", "temperature", "season",
                    "food", "restaurant", "cooking", "meal", "breakfast", "lunch", "dinner",
                    "book", "library", "reading", "study", "learn", "education", "school",
                    "movie", "music", "entertainment", "game", "sport", "exercise", "health",
                    "travel", "vacation", "country", "city", "park", "building", "street",
                    "family", "friend", "people", "person", "child", "adult", "student",
                    "work", "job", "business", "office", "meeting", "project", "task",
                    "money", "price", "shopping", "market", "store", "product", "service",
                    "car", "transport", "traffic", "road", "bridge", "airport", "station",
                    "phone", "computer", "internet", "website", "email", "message", "news",
                    "animal", "dog", "cat", "bird", "fish", "tree", "flower", "garden",
                    "mountain", "river", "ocean", "forest", "field", "sky", "sun", "moon",
                    "color", "red", "blue", "green", "yellow", "black", "white", "purple",
                    "big", "small", "good", "bad", "new", "old", "fast", "slow", "easy",
                    "happy", "sad", "angry", "excited", "tired", "hungry", "thirsty",
                    "time", "day", "night", "morning", "afternoon", "evening", "week",
                    "month", "year", "history", "future", "past", "present", "moment",
                    "idea", "thought", "feeling", "emotion", "dream", "goal", "plan",
                    "problem", "solution", "question", "answer", "reason", "cause", "effect",
                    "important", "necessary", "possible", "difficult", "simple", "complex"
                ]
                logger.info(f"âš ï¸ ä½¿ç”¨å¤‡ç”¨è¯åº“: {len(filtered_words)} ä¸ªå•è¯")
        
        except Exception as e:
            logger.error(f"âŒ è¯å…¸åŠ è½½å¤±è´¥: {e}")
            return []
        
        # éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„è¯æ±‡
        if len(filtered_words) >= count:
            return random.sample(filtered_words, count)
        else:
            # å¦‚æœè¯æ±‡ä¸å¤Ÿï¼Œé‡å¤ä½¿ç”¨
            multiplier = (count // len(filtered_words)) + 1
            extended_words = filtered_words * multiplier
            return random.sample(extended_words, count)
    
    def apply_mlm_masking_single(self, text: str, mask_ratio: float = 0.15) -> Dict:
        """å¯¹å•ä¸ªæ–‡æœ¬åº”ç”¨MLMæ©ç """
        tokens = self.tokenizer.tokenize(text)
        
        if len(tokens) < 3:  # å¤ªçŸ­çš„è·³è¿‡
            return None
        
        # ç¡®å®šæ©ç æ•°é‡
        num_masks = max(1, int(len(tokens) * mask_ratio))
        
        # éšæœºé€‰æ‹©æ©ç ä½ç½®
        mask_positions = random.sample(range(len(tokens)), min(num_masks, len(tokens)))
        
        original_tokens = tokens.copy()
        masked_tokens = tokens.copy()
        labels = [-100] * len(tokens)
        
        for pos in mask_positions:
            labels[pos] = self.tokenizer.convert_tokens_to_ids([original_tokens[pos]])[0]
            
            rand_val = random.random()
            if rand_val < 0.8:
                masked_tokens[pos] = '[MASK]'
            elif rand_val < 0.9:
                # æ›¿æ¢ä¸ºéšæœºtoken
                vocab_tokens = list(self.tokenizer.get_vocab().keys())
                masked_tokens[pos] = random.choice(vocab_tokens)
            # 10%ä¿æŒåŸæ ·
        
        masked_text = self.tokenizer.convert_tokens_to_string(masked_tokens)
        
        return {
            'original_text': text,
            'masked_text': masked_text,
            'mask_positions': mask_positions,
            'labels': labels
        }
    
    def save_paper_spec_datasets(self, mlm_samples: List[Dict], 
                                positive_pairs: List[Tuple[str, str]],
                                negative_pairs: List[Tuple[str, str]],
                                output_dir: str):
        """ä¿å­˜è®ºæ–‡è§„æ ¼æ•°æ®é›†"""
        logger.info(f"ğŸ’¾ ä¿å­˜è®ºæ–‡è§„æ ¼æ•°æ®é›†åˆ°: {output_dir}")
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ä¿å­˜MLMæ•°æ®
        mlm_df = pd.DataFrame(mlm_samples)
        mlm_file = output_path / "paper_spec_mlm_data.csv"
        mlm_df.to_csv(mlm_file, index=False)
        logger.info(f"âœ… MLMæ•°æ®: {len(mlm_samples)}ä¸ªæ ·æœ¬ -> {mlm_file}")
        
        # ä¿å­˜å¯¹æ¯”å­¦ä¹ æ•°æ®
        contrastive_data = []
        
        # æ·»åŠ æ­£æ ·æœ¬
        for text1, text2 in positive_pairs:
            contrastive_data.append({
                'text1': text1,
                'text2': text2,
                'label': 1
            })
        
        # æ·»åŠ è´Ÿæ ·æœ¬
        for text1, text2 in negative_pairs:
            contrastive_data.append({
                'text1': text1,
                'text2': text2,
                'label': 0
            })
        
        # éšæœºæ‰“ä¹±
        random.shuffle(contrastive_data)
        
        contrastive_df = pd.DataFrame(contrastive_data)
        contrastive_file = output_path / "paper_spec_contrastive_data.csv"
        contrastive_df.to_csv(contrastive_file, index=False)
        logger.info(f"âœ… å¯¹æ¯”å­¦ä¹ æ•°æ®: {len(contrastive_data)}ä¸ªæ ·æœ¬ -> {contrastive_file}")
        
        # ä¿å­˜é…ç½®ä¿¡æ¯
        config = {
            "paper_specification": {
                "mlm_samples": len(mlm_samples),
                "positive_pairs": len(positive_pairs),
                "negative_pairs": len(negative_pairs),
                "total_contrastive_pairs": len(positive_pairs) + len(negative_pairs)
            },
            "mlm_config": {
                "context_window": "3_preceding + 1_current + 3_subsequent",
                "mask_ratio": 0.15,
                "total_context_lines": 7
            },
            "contrastive_config": {
                "positive_strategy": "word_pairs_within_same_line",
                "negative_strategy": "code_words_with_general_domain_words",
                "token_filtering": "remove_punctuation_and_symbols",
                "min_token_length": 2
            },
            "joint_training": {
                "beta_ft": 0.7,
                "loss_function": "L_joint = Î²_ft * L_MLM + (1 - Î²_ft) * L_InfoNCE"
            }
        }
        
        config_file = output_path / "paper_spec_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        logger.info(f"âœ… é…ç½®ä¿¡æ¯: {config_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è®ºæ–‡è§„æ ¼æ•°æ®ç”Ÿæˆå™¨")
    parser.add_argument("--knowledge_base_dir", type=str, default="./knowledge_base",
                       help="çŸ¥è¯†åº“ç›®å½•")
    parser.add_argument("--tokenizer_path", type=str, default="./enhanced_communication_tokenizer",
                       help="Tokenizerè·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="paper_spec_training_data",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--mlm_samples", type=int, default=10285,
                       help="MLMæ ·æœ¬æ•°é‡")
    parser.add_argument("--contrastive_pairs", type=int, default=26609,
                       help="å¯¹æ¯”å­¦ä¹ æ ·æœ¬å¯¹æ•°é‡")
    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    logger.info("ğŸš€ å¯åŠ¨è®ºæ–‡è§„æ ¼æ•°æ®ç”Ÿæˆå™¨")
    logger.info(f"ğŸ“‹ é…ç½®å‚æ•°:")
    for key, value in vars(args).items():
        logger.info(f"  {key}: {value}")
    
    # åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨
    generator = PaperSpecDataGenerator(args.tokenizer_path, args.seed)
    
    # è·å–çŸ¥è¯†åº“æ–‡ä»¶
    kb_dir = Path(args.knowledge_base_dir)
    c_files = list(kb_dir.glob("*.c")) + list(kb_dir.glob("*.h"))
    
    if not c_files:
        logger.error(f"åœ¨{kb_dir}ä¸­æœªæ‰¾åˆ°Cä»£ç æ–‡ä»¶")
        return
    
    logger.info(f"ğŸ“ æ‰¾åˆ°{len(c_files)}ä¸ªä»£ç æ–‡ä»¶")
    
    # 1. ç”ŸæˆMLMæ ·æœ¬
    mlm_samples = generator.generate_mlm_samples(
        [str(f) for f in c_files],
        args.mlm_samples
    )
    
    # 2. ç”Ÿæˆå¯¹æ¯”å­¦ä¹ æ ·æœ¬å¯¹
    positive_pairs, negative_pairs = generator.generate_contrastive_pairs(
        [str(f) for f in c_files],
        args.contrastive_pairs
    )
    
    # 3. ä¿å­˜æ•°æ®é›†
    generator.save_paper_spec_datasets(
        mlm_samples, positive_pairs, negative_pairs, args.output_dir
    )
    
    # 4. æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    logger.info("ğŸ“Š è®ºæ–‡è§„æ ¼æ•°æ®é›†ç»Ÿè®¡:")
    logger.info(f"  MLMæ ·æœ¬: {len(mlm_samples)}")
    logger.info(f"  æ­£æ ·æœ¬å¯¹: {len(positive_pairs)}")
    logger.info(f"  è´Ÿæ ·æœ¬å¯¹: {len(negative_pairs)}")
    logger.info(f"  æ€»å¯¹æ¯”æ ·æœ¬: {len(positive_pairs) + len(negative_pairs)}")
    
    logger.info("ğŸ‰ è®ºæ–‡è§„æ ¼æ•°æ®ç”Ÿæˆå®Œæˆ!")

if __name__ == "__main__":
    main()
