#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºtokenizerå·¥å…·å‡½æ•°
æä¾›ç»Ÿä¸€çš„tokenizeråŠ è½½å’Œé…ç½®æ¥å£ï¼Œæ”¯æŒè‡ªåŠ¨åˆ›å»ºå¢å¼ºåˆ†è¯å™¨
"""

import os
import json
import shutil
import torch
from typing import Optional, Dict, Any, List
from transformers import AutoTokenizer, BertTokenizer, BertModel
import logging

logger = logging.getLogger(__name__)

class EnhancedTokenizerLoader:
    """å¢å¼ºtokenizeråŠ è½½å™¨"""
    
    # é€šä¿¡é¢†åŸŸä¸“ä¸šæœ¯è¯­åˆ—è¡¨
    # âš ï¸ ä¿®æ”¹æ­¤åˆ—è¡¨åï¼Œè¯·è°ƒç”¨ sync_enhanced_tokenizer() æ¥åŒæ­¥åˆ†è¯å™¨æ¨¡å‹
    COMMUNICATION_TERMS = [
        # 5G/NRæœ¯è¯­
        "timing_advance", "harq_processes", "aggregation_level",
        "scheduling_offset", "dci_format", "pdcch_config",
        "pucch_config", "srs_config", "csi_rs_config",
        "prach_config", "rach_config", "rnti_config",
        
        # LTEæœ¯è¯­  
        "enb_config", "cell_config", "ue_context",
        "radio_bearer", "eps_bearer", "qos_config",
        "measurement_config", "handover_config",
        
        # è°ƒåº¦ç›¸å…³
        "resource_allocation", "modulation_coding",
        "transport_block", "code_block", "rate_matching",
        "channel_coding", "scrambling_sequence",
        
        # ç‰©ç†å±‚
        "subcarrier_spacing", "cyclic_prefix", "guard_interval",
        "ofdm_symbol", "resource_element", "resource_block",
        "bandwidth_part", "carrier_frequency",
        
        # MACå±‚
        "mac_header", "mac_payload", "buffer_status",
        "scheduling_request", "random_access", "contention_resolution",
        
        # RLCå±‚
        "rlc_header", "sequence_number", "acknowledged_mode",
        "unacknowledged_mode", "transparent_mode",
        
        # PDCPå±‚
        "pdcp_header", "compression_config", "integrity_protection",
        "ciphering_config", "rohc_config",
        
        # RRCå±‚
        "rrc_connection", "system_information", "measurement_report",
        "capability_information", "security_config",
        
        # ç½‘ç»œæ¶æ„
        "core_network", "access_network", "transport_network",
        "network_function", "service_function",
        
        # åè®®æ ˆ
        "protocol_stack", "layer_interface", "service_access_point",
        "protocol_data_unit", "service_data_unit",
        
        # ä¿¡ä»¤æµç¨‹
        "attach_procedure", "detach_procedure", "registration_procedure",
        "authentication_procedure", "authorization_procedure",
        
        # QoSç›¸å…³
        "quality_of_service", "service_level_agreement",
        "traffic_shaping", "congestion_control",
        
        # ç§»åŠ¨æ€§ç®¡ç†
        "mobility_management", "location_management",
        "handover_management", "load_balancing",
        
        # å®‰å…¨ç›¸å…³
        "authentication_vector", "security_algorithm",
        "encryption_key", "integrity_key", "security_context",
        
        # æ— çº¿èµ„æºç®¡ç†
        "radio_resource_management", "interference_coordination",
        "power_control", "admission_control",
        
        # æ€§èƒ½ä¼˜åŒ–
        "throughput_optimization", "latency_optimization", 
        "energy_efficiency", "spectral_efficiency",
        
        # æµ‹è¯•å’Œæµ‹é‡
        "performance_monitoring", "key_performance_indicator",
        "network_optimization", "fault_management",
        
        # æ–°å¢æœ¯è¯­
        "beam_management", "massive_mimo", "network_slicing",
        "edge_computing", "ultra_reliable", "low_latency",
        "enhanced_mobile", "machine_type", "critical_communication",
        
        # ç®—æ³•ç›¸å…³
        "scheduling_algorithm", "resource_allocation_algorithm",
        "channel_estimation", "signal_processing",
        "error_correction", "adaptive_modulation",
        
        # æ•°æ®ç»“æ„
        "configuration_parameters", "measurement_parameters",
        "status_indication", "event_notification",
        "timer_configuration", "counter_management",
        
        # å‡½æ•°å’Œè¿‡ç¨‹
        "initialization_procedure", "configuration_procedure",
        "monitoring_procedure", "cleanup_procedure",
        "error_handling", "exception_processing",
        
        # ç‰¹å®šå®ç°
        "gnb_scheduler", "enb_scheduler", "mac_scheduler",
        "rlc_entity", "pdcp_entity", "rrc_entity",
        "nas_entity", "mm_entity", "sm_entity"
    ]
    
    @staticmethod
    def load_enhanced_tokenizer(
        enhanced_tokenizer_path: str = "./enhanced_communication_tokenizer",
        fallback_path: str = "bert-base-uncased",
        auto_create: bool = True
    ) -> AutoTokenizer:
        """
        åŠ è½½å¢å¼ºçš„é€šä¿¡é¢†åŸŸtokenizerï¼Œæ”¯æŒè‡ªåŠ¨åˆ›å»º
        
        Args:
            enhanced_tokenizer_path: å¢å¼ºtokenizerè·¯å¾„
            fallback_path: å›é€€tokenizerè·¯å¾„
            auto_create: æ˜¯å¦è‡ªåŠ¨åˆ›å»ºå¢å¼ºtokenizer
            
        Returns:
            AutoTokenizer: åŠ è½½çš„tokenizer
        """
        try:
            # æ£€æŸ¥å¢å¼ºtokenizeræ˜¯å¦å­˜åœ¨
            if os.path.exists(enhanced_tokenizer_path):
                vocab_file = os.path.join(enhanced_tokenizer_path, "vocab.txt")
                config_file = os.path.join(enhanced_tokenizer_path, "tokenizer_config.json")
                
                if os.path.exists(vocab_file) and os.path.exists(config_file):
                    logger.info(f"ğŸ“š åŠ è½½ç°æœ‰å¢å¼ºtokenizer: {enhanced_tokenizer_path}")
                    
                    # æ‰‹åŠ¨åˆ›å»ºtokenizer
                    tokenizer = BertTokenizer(
                        vocab_file=vocab_file,
                        do_lower_case=True,
                        strip_accents=None,
                        unk_token="[UNK]",
                        sep_token="[SEP]",
                        pad_token="[PAD]",
                        cls_token="[CLS]",
                        mask_token="[MASK]"
                    )
                    
                    # è¯»å–é…ç½®ä»¥è·å–è¯æ±‡è¡¨å¤§å°
                    with open(config_file, 'r') as f:
                        config = json.load(f)
                    
                    logger.info(f"âœ… å¢å¼ºtokenizeråŠ è½½æˆåŠŸ")
                    logger.info(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {config.get('vocab_size', len(tokenizer.vocab))}")
                    
                    return tokenizer
                else:
                    logger.warning(f"âš ï¸ å¢å¼ºtokenizeræ–‡ä»¶ä¸å®Œæ•´")
                    if auto_create:
                        logger.info("ğŸ”§ å°è¯•é‡æ–°åˆ›å»ºå¢å¼ºtokenizer...")
                        return EnhancedTokenizerLoader._auto_create_enhanced_tokenizer(
                            enhanced_tokenizer_path, fallback_path
                        )
            else:
                logger.warning(f"âš ï¸ å¢å¼ºtokenizerè·¯å¾„ä¸å­˜åœ¨: {enhanced_tokenizer_path}")
                if auto_create:
                    logger.info("ğŸš€ è‡ªåŠ¨åˆ›å»ºå¢å¼ºtokenizer...")
                    return EnhancedTokenizerLoader._auto_create_enhanced_tokenizer(
                        enhanced_tokenizer_path, fallback_path
                    )
                
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å¢å¼ºtokenizerå¤±è´¥: {e}")
            if auto_create:
                logger.info("ğŸ”§ å°è¯•è‡ªåŠ¨åˆ›å»º...")
                return EnhancedTokenizerLoader._auto_create_enhanced_tokenizer(
                    enhanced_tokenizer_path, fallback_path
                )
        
        # å›é€€åˆ°æ ‡å‡†tokenizer
        logger.info(f"ğŸ“š ä½¿ç”¨æ ‡å‡†tokenizer: {fallback_path}")
        return AutoTokenizer.from_pretrained(fallback_path)
    
    @staticmethod
    def load_extended_embeddings(
        embedding_path: str = "./enhanced_communication_tokenizer/extended_embeddings_v2.pt",
        device: str = "cpu"
    ) -> Optional[torch.Tensor]:
        """
        åŠ è½½æ‰©å±•çš„åµŒå…¥çŸ©é˜µ
        
        Args:
            embedding_path: åµŒå…¥æ–‡ä»¶è·¯å¾„
            device: ç›®æ ‡è®¾å¤‡
            
        Returns:
            torch.Tensor: æ‰©å±•çš„åµŒå…¥çŸ©é˜µï¼Œå¦‚æœåŠ è½½å¤±è´¥è¿”å›None
        """
        try:
            if os.path.exists(embedding_path):
                logger.info(f"ğŸ§  åŠ è½½æ‰©å±•åµŒå…¥: {embedding_path}")
                embeddings = torch.load(embedding_path, map_location=device, weights_only=True)
                logger.info(f"âœ… åµŒå…¥çŸ©é˜µåŠ è½½æˆåŠŸ: {embeddings.shape}")
                return embeddings
            else:
                logger.warning(f"âš ï¸ åµŒå…¥æ–‡ä»¶ä¸å­˜åœ¨: {embedding_path}")
                
        except Exception as e:
            logger.error(f"âŒ åŠ è½½åµŒå…¥çŸ©é˜µå¤±è´¥: {e}")
        
        return None
    
    @staticmethod
    def get_decomposition_mapping(
        mapping_path: str = "./enhanced_communication_tokenizer/decomposition_mapping.json"
    ) -> Optional[Dict[str, Any]]:
        """
        è·å–æœ¯è¯­åˆ†è§£æ˜ å°„
        
        Args:
            mapping_path: æ˜ å°„æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: åˆ†è§£æ˜ å°„å­—å…¸ï¼Œå¦‚æœåŠ è½½å¤±è´¥è¿”å›None
        """
        try:
            if os.path.exists(mapping_path):
                logger.info(f"ğŸ“ åŠ è½½åˆ†è§£æ˜ å°„: {mapping_path}")
                with open(mapping_path, 'r', encoding='utf-8') as f:
                    mapping = json.load(f)
                logger.info(f"âœ… åˆ†è§£æ˜ å°„åŠ è½½æˆåŠŸ: {len(mapping)} ä¸ªæœ¯è¯­")
                return mapping
            else:
                logger.warning(f"âš ï¸ æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {mapping_path}")
                
        except Exception as e:
            logger.error(f"âŒ åŠ è½½åˆ†è§£æ˜ å°„å¤±è´¥: {e}")
        
        return None
    
    @staticmethod
    def get_tokenizer_info(tokenizer: AutoTokenizer) -> Dict[str, Any]:
        """
        è·å–tokenizerä¿¡æ¯
        
        Args:
            tokenizer: tokenizerå¯¹è±¡
            
        Returns:
            Dict: tokenizerä¿¡æ¯
        """
        info = {
            "vocab_size": len(tokenizer.vocab) if hasattr(tokenizer, 'vocab') else tokenizer.vocab_size,
            "model_max_length": tokenizer.model_max_length,
            "special_tokens": {
                "pad_token": tokenizer.pad_token,
                "unk_token": tokenizer.unk_token,
                "cls_token": tokenizer.cls_token,
                "sep_token": tokenizer.sep_token,
                "mask_token": tokenizer.mask_token,
            }
        }
        return info

    @staticmethod
    def _auto_create_enhanced_tokenizer(
        enhanced_tokenizer_path: str,
        base_tokenizer_path: str
    ) -> AutoTokenizer:
        """
        è‡ªåŠ¨åˆ›å»ºå¢å¼ºtokenizer
        
        Args:
            enhanced_tokenizer_path: å¢å¼ºtokenizerè¾“å‡ºè·¯å¾„
            base_tokenizer_path: åŸºç¡€tokenizerè·¯å¾„
            
        Returns:
            AutoTokenizer: åˆ›å»ºçš„å¢å¼ºtokenizer
        """
        try:
            logger.info("ğŸ› ï¸ å¼€å§‹åˆ›å»ºå¢å¼ºé€šä¿¡tokenizer...")
            
            # æ£€æŸ¥åŸºç¡€tokenizerè·¯å¾„
            if not os.path.exists(base_tokenizer_path):
                # å°è¯•ä»é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½
                logger.info(f"ğŸ“¥ ä»HuggingFaceä¸‹è½½åŸºç¡€tokenizer: {base_tokenizer_path}")
                base_tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_path)
                # è¿™é‡Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ä¸‹è½½çš„tokenizerï¼Œä¸ä¿å­˜åˆ°æœ¬åœ°
            else:
                base_tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_path)
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(enhanced_tokenizer_path, exist_ok=True)
            
            # 1. å¤åˆ¶åŸºç¡€tokenizeræ–‡ä»¶ï¼ˆå¦‚æœæœ¬åœ°å­˜åœ¨ï¼‰
            if os.path.exists(base_tokenizer_path):
                EnhancedTokenizerLoader._copy_base_tokenizer_files(
                    base_tokenizer_path, enhanced_tokenizer_path
                )
            else:
                # ä¿å­˜ä¸‹è½½çš„tokenizeræ–‡ä»¶
                base_tokenizer.save_pretrained(enhanced_tokenizer_path)
            
            # 2. æ‰©å±•è¯æ±‡è¡¨
            original_vocab_size = EnhancedTokenizerLoader._extend_vocabulary(
                enhanced_tokenizer_path, base_tokenizer
            )
            
            # 3. åˆ›å»ºæ‰©å±•åµŒå…¥çŸ©é˜µ
            EnhancedTokenizerLoader._create_extended_embeddings(
                enhanced_tokenizer_path, base_tokenizer_path, original_vocab_size
            )
            
            # 4. åˆ›å»ºæœ¯è¯­åˆ†è§£æ˜ å°„
            EnhancedTokenizerLoader._create_decomposition_mapping(
                enhanced_tokenizer_path, base_tokenizer
            )
            
            # 5. æ›´æ–°é…ç½®æ–‡ä»¶
            EnhancedTokenizerLoader._update_tokenizer_config(enhanced_tokenizer_path)
            
            # 6. åŒæ­¥tokenizer.jsonæ–‡ä»¶
            EnhancedTokenizerLoader._sync_tokenizer_json(enhanced_tokenizer_path)
            
            # 7. åŠ è½½æ–°åˆ›å»ºçš„tokenizer
            tokenizer = BertTokenizer.from_pretrained(enhanced_tokenizer_path)
            
            logger.info("âœ… å¢å¼ºtokenizeråˆ›å»ºæˆåŠŸï¼")
            logger.info(f"ğŸ“Š æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {len(tokenizer.vocab)}")
            
            return tokenizer
            
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨åˆ›å»ºå¢å¼ºtokenizerå¤±è´¥: {e}")
            logger.info(f"ğŸ”„ å›é€€åˆ°æ ‡å‡†tokenizer: {base_tokenizer_path}")
            return AutoTokenizer.from_pretrained(base_tokenizer_path)
    
    @staticmethod
    def _copy_base_tokenizer_files(base_path: str, output_path: str):
        """å¤åˆ¶åŸºç¡€tokenizeræ–‡ä»¶"""
        logger.info("ğŸ“ å¤åˆ¶åŸºç¡€tokenizeræ–‡ä»¶...")
        
        base_files = ['tokenizer.json', 'tokenizer_config.json', 'vocab.txt']
        
        for file_name in base_files:
            src_path = os.path.join(base_path, file_name)
            dst_path = os.path.join(output_path, file_name)
            
            if os.path.exists(src_path):
                shutil.copy2(src_path, dst_path)
                logger.info(f"  âœ“ å¤åˆ¶: {file_name}")
    
    @staticmethod
    def _extend_vocabulary(output_path: str, base_tokenizer: AutoTokenizer) -> int:
        """æ‰©å±•è¯æ±‡è¡¨"""
        logger.info("ğŸ“š æ‰©å±•è¯æ±‡è¡¨...")
        
        vocab_path = os.path.join(output_path, 'vocab.txt')
        
        # è¯»å–åŸå§‹è¯æ±‡è¡¨
        with open(vocab_path, 'r', encoding='utf-8') as f:
            original_vocab = [line.strip() for line in f]
        
        original_size = len(original_vocab)
        logger.info(f"  åŸå§‹è¯æ±‡è¡¨å¤§å°: {original_size}")
        
        # è·å–ç°æœ‰è¯æ±‡é›†åˆ
        existing_vocab = set(original_vocab)
        
        # æ·»åŠ æ–°æœ¯è¯­ï¼ˆæ£€æŸ¥é‡å¤ï¼‰
        new_terms = []
        for term in EnhancedTokenizerLoader.COMMUNICATION_TERMS:
            if term not in existing_vocab:
                new_terms.append(term)
                existing_vocab.add(term)
        
        # å†™å…¥æ‰©å±•åçš„è¯æ±‡è¡¨
        extended_vocab = original_vocab + new_terms
        
        with open(vocab_path, 'w', encoding='utf-8') as f:
            for word in extended_vocab:
                f.write(f"{word}\n")
        
        logger.info(f"  æ–°å¢æœ¯è¯­: {len(new_terms)} ä¸ª")
        logger.info(f"  æ‰©å±•åè¯æ±‡è¡¨å¤§å°: {len(extended_vocab)}")
        
        return original_size
    
    @staticmethod
    def _create_extended_embeddings(output_path: str, base_path: str, original_vocab_size: int):
        """åˆ›å»ºæ‰©å±•åµŒå…¥çŸ©é˜µ"""
        logger.info("ğŸ§  åˆ›å»ºæ‰©å±•åµŒå…¥çŸ©é˜µ...")
        
        try:
            # å°è¯•åŠ è½½åŸå§‹æ¨¡å‹ä»¥è·å–åµŒå…¥çŸ©é˜µ
            if os.path.exists(base_path):
                bert_model = BertModel.from_pretrained(base_path)
            else:
                bert_model = BertModel.from_pretrained("bert-base-uncased")
                
            original_embeddings = bert_model.embeddings.word_embeddings.weight.data
            
            embedding_dim = original_embeddings.shape[1]
            new_vocab_size = original_vocab_size + len(EnhancedTokenizerLoader.COMMUNICATION_TERMS)
            
            # åˆ›å»ºæ‰©å±•åµŒå…¥çŸ©é˜µ
            extended_embeddings = torch.zeros(new_vocab_size, embedding_dim)
            
            # å¤åˆ¶åŸå§‹åµŒå…¥
            extended_embeddings[:original_vocab_size] = original_embeddings[:original_vocab_size]
            
            # ä¸ºæ–°æœ¯è¯­åˆå§‹åŒ–åµŒå…¥ï¼ˆä½¿ç”¨æ­£æ€åˆ†å¸ƒï¼‰
            std = original_embeddings.std().item()
            extended_embeddings[original_vocab_size:] = torch.normal(
                mean=0.0, std=std, 
                size=(len(EnhancedTokenizerLoader.COMMUNICATION_TERMS), embedding_dim)
            )
            
            # ä¿å­˜æ‰©å±•åµŒå…¥
            embedding_path = os.path.join(output_path, 'extended_embeddings_v2.pt')
            torch.save(extended_embeddings, embedding_path)
            
            logger.info(f"  âœ“ åµŒå…¥çŸ©é˜µ: {extended_embeddings.shape}")
            
        except Exception as e:
            logger.warning(f"  âš ï¸ åˆ›å»ºåµŒå…¥çŸ©é˜µå¤±è´¥: {e}")
    
    @staticmethod
    def _create_decomposition_mapping(output_path: str, base_tokenizer: AutoTokenizer):
        """åˆ›å»ºæœ¯è¯­åˆ†è§£æ˜ å°„"""
        logger.info("ğŸ“ åˆ›å»ºæœ¯è¯­åˆ†è§£æ˜ å°„...")
        
        decomposition_mapping = {}
        
        for term in EnhancedTokenizerLoader.COMMUNICATION_TERMS:
            tokens = base_tokenizer.tokenize(term)
            
            decomposition_mapping[term] = {
                "original_tokens": tokens,
                "token_count": len(tokens),
                "improved_tokenization": [term],
                "improvement_ratio": len(tokens)
            }
        
        mapping_path = os.path.join(output_path, 'decomposition_mapping.json')
        with open(mapping_path, 'w', encoding='utf-8') as f:
            json.dump(decomposition_mapping, f, indent=2, ensure_ascii=False)
        
        logger.info(f"  âœ“ æœ¯è¯­æ˜ å°„: {len(decomposition_mapping)} ä¸ª")
    
    @staticmethod
    def _update_tokenizer_config(output_path: str):
        """æ›´æ–°tokenizeré…ç½®"""
        logger.info("âš™ï¸ æ›´æ–°tokenizeré…ç½®...")
        
        config_path = os.path.join(output_path, 'tokenizer_config.json')
        
        # è¯»å–åŸå§‹é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # æ›´æ–°é…ç½®
        config.update({
            "enhanced_tokenizer": True,
            "communication_domain": True,
            "added_tokens": len(EnhancedTokenizerLoader.COMMUNICATION_TERMS),
            "vocab_size": 30522 + len(EnhancedTokenizerLoader.COMMUNICATION_TERMS),
            "model_type": "enhanced_bert_communication",
            "domain": "wireless_communication",
            "version": "v2.0",
            "auto_created": True
        })
        
        # ä¿å­˜æ›´æ–°åçš„é…ç½®
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
    
    @staticmethod
    def _sync_tokenizer_json(output_path: str):
        """åŒæ­¥æ›´æ–° tokenizer.json ä»¥åŒ¹é…æ‰©å±•çš„è¯æ±‡è¡¨"""
        logger.info("ğŸ”„ åŒæ­¥tokenizer.json...")
        
        vocab_path = os.path.join(output_path, 'vocab.txt')
        tokenizer_json_path = os.path.join(output_path, 'tokenizer.json')
        
        # è¯»å–å½“å‰è¯æ±‡è¡¨
        with open(vocab_path, 'r', encoding='utf-8') as f:
            vocab_list = [line.strip() for line in f if line.strip()]
        
        # åŠ è½½ç°æœ‰çš„ tokenizer.json
        with open(tokenizer_json_path, 'r', encoding='utf-8') as f:
            tokenizer_data = json.load(f)
        
        # åˆ›å»ºæ–°çš„è¯æ±‡æ˜ å°„
        from collections import OrderedDict
        new_vocab = OrderedDict()
        for idx, token in enumerate(vocab_list):
            new_vocab[token] = idx
        
        # æ›´æ–° tokenizer.json ä¸­çš„è¯æ±‡è¡¨
        if 'model' not in tokenizer_data:
            tokenizer_data['model'] = {}
        
        tokenizer_data['model']['vocab'] = new_vocab
        
        # æ›´æ–° added_tokensï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'added_tokens' in tokenizer_data:
            # ç¡®ä¿æ‰€æœ‰æ–°å¢çš„é€šä¿¡æœ¯è¯­éƒ½åœ¨ added_tokens ä¸­
            existing_added = {token['content'] for token in tokenizer_data.get('added_tokens', [])}
            
            # ä»è¯æ±‡è¡¨æœ«å°¾æå–æ–°å¢çš„æœ¯è¯­
            original_vocab_size = 30522
            new_tokens = vocab_list[original_vocab_size:]
            
            for token in new_tokens:
                if token not in existing_added:
                    tokenizer_data['added_tokens'].append({
                        "id": vocab_list.index(token),
                        "content": token,
                        "single_word": True,
                        "lstrip": False,
                        "rstrip": False,
                        "normalized": False,
                        "special": False
                    })
        
        # ç¡®ä¿æ¨¡å‹ç±»å‹æ­£ç¡®
        if 'model' in tokenizer_data:
            if 'type' not in tokenizer_data['model']:
                tokenizer_data['model']['type'] = 'WordPiece'
            
            # æ·»åŠ å…¶ä»–å¿…è¦çš„æ¨¡å‹é…ç½®
            if 'unk_token' not in tokenizer_data['model']:
                tokenizer_data['model']['unk_token'] = '[UNK]'
            if 'sep_token' not in tokenizer_data['model']:
                tokenizer_data['model']['sep_token'] = '[SEP]'
            if 'cls_token' not in tokenizer_data['model']:
                tokenizer_data['model']['cls_token'] = '[CLS]'
        
        # ä¿å­˜æ›´æ–°åçš„ tokenizer.json
        with open(tokenizer_json_path, 'w', encoding='utf-8') as f:
            json.dump(tokenizer_data, f, ensure_ascii=False, separators=(',', ':'))
        
        logger.info(f"  âœ“ tokenizer.jsonå·²åŒæ­¥")
        logger.info(f"  âœ“ è¯æ±‡è¡¨å¤§å°: {len(new_vocab)}")
        
        return len(new_vocab)

def get_enhanced_tokenizer(enhanced_path: str = None, auto_create: bool = True) -> AutoTokenizer:
    """
    ä¾¿æ·å‡½æ•°ï¼šè·å–å¢å¼ºtokenizerï¼Œæ”¯æŒè‡ªåŠ¨åˆ›å»º
    
    Args:
        enhanced_path: å¢å¼ºtokenizerè·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨æ ‡å‡†è·¯å¾„
        auto_create: æ˜¯å¦è‡ªåŠ¨åˆ›å»ºå¢å¼ºtokenizer
        
    Returns:
        AutoTokenizer: å¢å¼ºtokenizer
    """
    if enhanced_path is None:
        enhanced_path = "./enhanced_communication_tokenizer"
    
    return EnhancedTokenizerLoader.load_enhanced_tokenizer(
        enhanced_path, auto_create=auto_create
    )

def sync_enhanced_tokenizer(enhanced_path: str = None) -> bool:
    """
    æ‰‹åŠ¨åŒæ­¥å¢å¼ºåˆ†è¯å™¨çš„tokenizer.jsonæ–‡ä»¶
    å½“ä¿®æ”¹äº†COMMUNICATION_TERMSçŸ¥è¯†åº“åï¼Œä½¿ç”¨æ­¤å‡½æ•°ç¡®ä¿åˆ†è¯å™¨æ¨¡å‹åŒæ­¥
    
    Args:
        enhanced_path: å¢å¼ºtokenizerè·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨æ ‡å‡†è·¯å¾„
        
    Returns:
        bool: åŒæ­¥æ˜¯å¦æˆåŠŸ
    """
    if enhanced_path is None:
        enhanced_path = "./enhanced_communication_tokenizer"
    
    try:
        logger.info("ğŸ”„ æ‰‹åŠ¨åŒæ­¥å¢å¼ºåˆ†è¯å™¨...")
        
        # æ£€æŸ¥å¢å¼ºåˆ†è¯å™¨æ˜¯å¦å­˜åœ¨
        if not os.path.exists(enhanced_path):
            logger.error(f"âŒ å¢å¼ºåˆ†è¯å™¨ç›®å½•ä¸å­˜åœ¨: {enhanced_path}")
            return False
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        required_files = ['vocab.txt', 'tokenizer.json']
        for file in required_files:
            file_path = os.path.join(enhanced_path, file)
            if not os.path.exists(file_path):
                logger.error(f"âŒ å¿…è¦æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
        
        # æ‰§è¡ŒåŒæ­¥
        vocab_size = EnhancedTokenizerLoader._sync_tokenizer_json(enhanced_path)
        
        # éªŒè¯åŒæ­¥ç»“æœ
        try:
            from transformers import BertTokenizer
            tokenizer = BertTokenizer.from_pretrained(enhanced_path)
            
            if tokenizer.vocab_size == vocab_size:
                logger.info("âœ… åˆ†è¯å™¨åŒæ­¥æˆåŠŸï¼")
                logger.info(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
                
                # å¿«é€Ÿæµ‹è¯•å‡ ä¸ªæœ¯è¯­
                test_terms = ["aggregation_level", "timing_advance", "transport_block"]
                for term in test_terms:
                    tokens = tokenizer.tokenize(term)
                    if len(tokens) == 1 and tokens[0] == term:
                        logger.info(f"  âœ“ {term} -> {tokens}")
                    else:
                        logger.warning(f"  âš ï¸ {term} -> {tokens} (æœªä¼˜åŒ–)")
                
                return True
            else:
                logger.error(f"âŒ è¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…: æœŸæœ›{vocab_size}, å®é™…{tokenizer.vocab_size}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ éªŒè¯åŒæ­¥ç»“æœå¤±è´¥: {e}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ åŒæ­¥å¤±è´¥: {e}")
        return False

def get_extended_embeddings(embedding_path: str = None, device: str = "cpu") -> Optional[torch.Tensor]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè·å–æ‰©å±•åµŒå…¥
    
    Args:
        embedding_path: åµŒå…¥æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨æ ‡å‡†è·¯å¾„
        device: ç›®æ ‡è®¾å¤‡
        
    Returns:
        torch.Tensor: æ‰©å±•åµŒå…¥çŸ©é˜µ
    """
    if embedding_path is None:
        embedding_path = "./enhanced_communication_tokenizer/extended_embeddings_v2.pt"
    
    return EnhancedTokenizerLoader.load_extended_embeddings(embedding_path, device)
