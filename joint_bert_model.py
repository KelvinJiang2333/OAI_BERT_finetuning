#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è”åˆMLM+å¯¹æ¯”å­¦ä¹ çš„BERTæ¨¡å‹æ¶æ„

å®ç°è®ºæ–‡ä¸­çš„è”åˆä¼˜åŒ–æ¡†æ¶ï¼š
- MLM (Masked Language Modeling) ä»»åŠ¡
- å¯¹æ¯”å­¦ä¹  (Contrastive Learning) ä»»åŠ¡
- è”åˆæŸå¤±å‡½æ•°ï¼šL_joint = Î²_ft * L_MLM + (1 - Î²_ft) * L_InfoNCE
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertModel, BertConfig, BertTokenizer
from typing import Dict, Optional, Tuple
import logging
from tokenizer_utils import get_enhanced_tokenizer, get_extended_embeddings

logger = logging.getLogger(__name__)

class JointBertModel(nn.Module):
    """
    è”åˆMLM+å¯¹æ¯”å­¦ä¹ çš„BERTæ¨¡å‹
    
    æ¶æ„ï¼š
    1. å…±äº«çš„BERTç¼–ç å™¨
    2. MLMå¤´ï¼šç”¨äºæ©ç è¯­è¨€å»ºæ¨¡
    3. å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´ï¼šç”¨äºç”Ÿæˆå¯¹æ¯”å­¦ä¹ åµŒå…¥
    """
    
    def __init__(self, 
                 bert_model_path: str,
                 contrastive_dim: int = 128,
                 dropout_rate: float = 0.1,
                 temperature: float = 0.07,
                 use_enhanced_tokenizer: bool = False):
        """
        åˆå§‹åŒ–è”åˆæ¨¡å‹
        
        Args:
            bert_model_path: BERTæ¨¡å‹è·¯å¾„
            contrastive_dim: å¯¹æ¯”å­¦ä¹ åµŒå…¥ç»´åº¦
            dropout_rate: Dropoutç‡
            temperature: å¯¹æ¯”å­¦ä¹ æ¸©åº¦å‚æ•°
            use_enhanced_tokenizer: æ˜¯å¦ä½¿ç”¨å¢å¼ºtokenizer
        """
        super().__init__()
        
        # åŠ è½½BERTé…ç½®å’Œæ¨¡å‹
        self.bert_config = BertConfig.from_pretrained(bert_model_path)
        self.bert = BertModel.from_pretrained(bert_model_path)
        
        # è·å–BERTéšè—å±‚ç»´åº¦
        self.hidden_size = self.bert_config.hidden_size
        self.vocab_size = self.bert_config.vocab_size
        self.contrastive_dim = contrastive_dim
        self.temperature = temperature
        self.dropout_rate = dropout_rate
        
        # å¦‚æœä½¿ç”¨å¢å¼ºtokenizerï¼Œæ‰©å±•è¯æ±‡è¡¨
        if use_enhanced_tokenizer:
            self._setup_enhanced_tokenizer()
        
        # åˆå§‹åŒ–æ¨¡å‹å¤´éƒ¨
        self._initialize_heads()
        
    def _setup_enhanced_tokenizer(self):
        """è®¾ç½®å¢å¼ºtokenizerå’Œæ‰©å±•åµŒå…¥"""
        try:
            logger.info("ğŸ”§ è®¾ç½®å¢å¼ºtokenizer...")
            extended_embeddings = get_extended_embeddings()
            
            if extended_embeddings is not None:
                current_vocab_size = self.bert.config.vocab_size
                new_vocab_size = extended_embeddings.shape[0]
                
                if new_vocab_size > current_vocab_size:
                    logger.info(f"ğŸ“ˆ æ‰©å±•è¯æ±‡è¡¨: {current_vocab_size} â†’ {new_vocab_size}")
                    
                    # åˆ›å»ºæ–°çš„åµŒå…¥å±‚
                    new_embeddings = torch.nn.Embedding(
                        new_vocab_size, 
                        extended_embeddings.shape[1]
                    )
                    new_embeddings.weight.data = extended_embeddings
                    
                    # æ›¿æ¢åŸæœ‰åµŒå…¥å±‚
                    self.bert.embeddings.word_embeddings = new_embeddings
                    
                    # æ›´æ–°é…ç½®
                    self.bert.config.vocab_size = new_vocab_size
                    self.vocab_size = new_vocab_size  # æ›´æ–°å®ä¾‹å˜é‡
                    
                    logger.info("âœ… å¢å¼ºtokenizerè®¾ç½®å®Œæˆ")
                else:
                    logger.warning("âš ï¸ æ‰©å±•åµŒå…¥å¤§å°ä¸åŒ¹é…")
            else:
                logger.warning("âš ï¸ æ— æ³•åŠ è½½æ‰©å±•åµŒå…¥")
                
        except Exception as e:
            logger.error(f"âŒ è®¾ç½®å¢å¼ºtokenizerå¤±è´¥: {e}")
    
    def _initialize_heads(self):
        """åˆå§‹åŒ–æ¨¡å‹å¤´éƒ¨"""
        # MLMå¤´ï¼šç”¨äºé¢„æµ‹æ©ç token (åœ¨tokenizerè®¾ç½®ååˆ›å»º)
        self.mlm_head = nn.Linear(self.hidden_size, self.vocab_size)
        
        # å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´ï¼šå°†BERTåµŒå…¥æ˜ å°„åˆ°å¯¹æ¯”å­¦ä¹ ç©ºé—´
        self.contrastive_projection = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.ReLU(),
            nn.Dropout(self.dropout_rate),
            nn.Linear(self.hidden_size, self.contrastive_dim)
        )
        
        # å‚æ•°å·²åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
        
        logger.info(f"åˆå§‹åŒ–JointBertModel - éšè—ç»´åº¦: {self.hidden_size}, å¯¹æ¯”å­¦ä¹ ç»´åº¦: {self.contrastive_dim}")
    
    def _init_weights(self):
        """åˆå§‹åŒ–æ–°å¢å±‚çš„æƒé‡"""
        # MLMå¤´æƒé‡åˆå§‹åŒ–
        nn.init.normal_(self.mlm_head.weight, mean=0.0, std=self.bert_config.initializer_range)
        nn.init.zeros_(self.mlm_head.bias)
        
        # å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´æƒé‡åˆå§‹åŒ–
        for module in self.contrastive_projection:
            if isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, mean=0.0, std=self.bert_config.initializer_range)
                nn.init.zeros_(module.bias)
    
    def forward(self, 
                input_ids: torch.Tensor,
                attention_mask: torch.Tensor,
                token_type_ids: Optional[torch.Tensor] = None,
                mlm_labels: Optional[torch.Tensor] = None,
                contrastive_labels: Optional[torch.Tensor] = None,
                return_embeddings: bool = False) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_ids: è¾“å…¥token IDs [batch_size, seq_len]
            attention_mask: æ³¨æ„åŠ›æ©ç  [batch_size, seq_len]
            token_type_ids: tokenç±»å‹IDs [batch_size, seq_len]
            mlm_labels: MLMæ ‡ç­¾ [batch_size, seq_len]ï¼Œ-100è¡¨ç¤ºéæ©ç ä½ç½®
            contrastive_labels: å¯¹æ¯”å­¦ä¹ æ ‡ç­¾ [batch_size]ï¼Œ1è¡¨ç¤ºæ­£æ ·æœ¬å¯¹ï¼Œ0è¡¨ç¤ºè´Ÿæ ·æœ¬å¯¹
            return_embeddings: æ˜¯å¦è¿”å›åµŒå…¥å‘é‡
            
        Returns:
            DictåŒ…å«ï¼š
            - mlm_logits: MLMé¢„æµ‹logits [batch_size, seq_len, vocab_size]
            - contrastive_embeddings: å¯¹æ¯”å­¦ä¹ åµŒå…¥ [batch_size, contrastive_dim]
            - mlm_loss: MLMæŸå¤± (å¦‚æœæä¾›mlm_labels)
            - contrastive_loss: å¯¹æ¯”å­¦ä¹ æŸå¤± (å¦‚æœæä¾›contrastive_labels)
            - joint_loss: è”åˆæŸå¤±
        """
        outputs = {}
        
        # é€šè¿‡å…±äº«BERTç¼–ç å™¨
        bert_outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            return_dict=True
        )
        
        # è·å–åºåˆ—è¾“å‡ºå’Œæ± åŒ–è¾“å‡º
        sequence_output = bert_outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]
        pooled_output = bert_outputs.pooler_output  # [batch_size, hidden_size]
        
        # MLMä»»åŠ¡
        mlm_logits = self.mlm_head(sequence_output)  # [batch_size, seq_len, vocab_size]
        outputs['mlm_logits'] = mlm_logits
        
        # å¯¹æ¯”å­¦ä¹ ä»»åŠ¡
        # ä½¿ç”¨[CLS]tokençš„è¡¨ç¤ºè¿›è¡Œå¯¹æ¯”å­¦ä¹ 
        cls_embeddings = sequence_output[:, 0, :]  # [batch_size, hidden_size]
        contrastive_embeddings = self.contrastive_projection(cls_embeddings)  # [batch_size, contrastive_dim]
        # L2å½’ä¸€åŒ–
        contrastive_embeddings = F.normalize(contrastive_embeddings, p=2, dim=1)
        outputs['contrastive_embeddings'] = contrastive_embeddings
        
        # å¦‚æœéœ€è¦è¿”å›åŸå§‹åµŒå…¥
        if return_embeddings:
            outputs['cls_embeddings'] = cls_embeddings
            outputs['sequence_embeddings'] = sequence_output
        
        # è®¡ç®—æŸå¤±
        total_loss = 0.0
        
        # MLMæŸå¤±
        if mlm_labels is not None:
            mlm_loss = self._compute_mlm_loss(mlm_logits, mlm_labels)
            outputs['mlm_loss'] = mlm_loss
            total_loss = total_loss + mlm_loss  # é¿å…inplaceæ“ä½œ
        
        # å¯¹æ¯”å­¦ä¹ æŸå¤±
        if contrastive_labels is not None:
            contrastive_loss = self._compute_contrastive_loss(contrastive_embeddings, contrastive_labels)
            outputs['contrastive_loss'] = contrastive_loss
            total_loss = total_loss + contrastive_loss  # é¿å…inplaceæ“ä½œ
        
        if mlm_labels is not None or contrastive_labels is not None:
            outputs['joint_loss'] = total_loss
        
        return outputs
    
    def _compute_mlm_loss(self, mlm_logits: torch.Tensor, mlm_labels: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—MLMæŸå¤±
        
        Args:
            mlm_logits: MLMé¢„æµ‹logits [batch_size, seq_len, vocab_size]
            mlm_labels: MLMæ ‡ç­¾ [batch_size, seq_len]ï¼Œ-100è¡¨ç¤ºéæ©ç ä½ç½®
            
        Returns:
            MLMæŸå¤±
        """
        # é‡å¡‘å¼ é‡
        mlm_logits = mlm_logits.view(-1, self.vocab_size)
        mlm_labels = mlm_labels.view(-1)
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼Œå¿½ç•¥-100æ ‡ç­¾
        loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
        mlm_loss = loss_fct(mlm_logits, mlm_labels)
        
        return mlm_loss
    
    def _compute_contrastive_loss(self, embeddings: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—æ”¹è¿›çš„InfoNCEå¯¹æ¯”å­¦ä¹ æŸå¤±
        
        Args:
            embeddings: å½’ä¸€åŒ–çš„åµŒå…¥å‘é‡ [batch_size, contrastive_dim]
            labels: å¯¹æ¯”å­¦ä¹ æ ‡ç­¾ [batch_size]ï¼Œ1è¡¨ç¤ºæ­£æ ·æœ¬ï¼Œ0è¡¨ç¤ºè´Ÿæ ·æœ¬
            
        Returns:
            å¯¹æ¯”å­¦ä¹ æŸå¤±
        """
        batch_size = embeddings.shape[0]
        device = embeddings.device
        
        # 1. ç¡®ä¿embeddingså·²å½’ä¸€åŒ–
        embeddings = F.normalize(embeddings, p=2, dim=1)
        
        # 2. åŠ¨æ€è°ƒæ•´æ¸©åº¦å‚æ•°ï¼ˆå¦‚æœæŸå¤±è¿‡é«˜ï¼‰
        # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„è‡ªé€‚åº”ç­–ç•¥
        effective_temperature = max(self.temperature, 0.1)  # é¿å…æ¸©åº¦è¿‡ä½
        
        # 3. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.matmul(embeddings, embeddings.T) / effective_temperature
        
        # 4. åˆ›å»ºæ©ç ï¼Œæ’é™¤è‡ªèº«å¯¹è§’çº¿
        eye_mask = torch.eye(batch_size, device=device).bool()
        
        # 5. æ”¹è¿›çš„æ­£æ ·æœ¬å¯¹è¯†åˆ« - æ”¯æŒä¸åŒçš„æ ‡ç­¾ç­–ç•¥
        if labels.dtype == torch.bool or labels.max() <= 1:
            # äºŒåˆ†ç±»æ ‡ç­¾ï¼š1è¡¨ç¤ºæ­£æ ·æœ¬ç±»ï¼Œ0è¡¨ç¤ºè´Ÿæ ·æœ¬ç±»
            positive_mask = (labels.unsqueeze(0) == labels.unsqueeze(1)) & (labels.unsqueeze(1) == 1)
        else:
            # å¤šåˆ†ç±»æ ‡ç­¾ï¼šç›¸åŒç±»åˆ«ä¸ºæ­£æ ·æœ¬å¯¹
            positive_mask = labels.unsqueeze(0) == labels.unsqueeze(1)
        
        positive_mask = positive_mask & (~eye_mask)  # æ’é™¤è‡ªèº«
        
        # 6. è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±
        losses = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ­£æ ·æœ¬å¯¹
        total_positive_pairs = positive_mask.sum().item()
        if total_positive_pairs == 0:
            # æ²¡æœ‰æ­£æ ·æœ¬å¯¹æ—¶ï¼Œä½¿ç”¨ä¿®æ”¹çš„ç­–ç•¥
            # é¼“åŠ±åŒç±»æ ·æœ¬ç›¸ä¼¼ï¼Œä¸åŒç±»æ ·æœ¬ä¸ç›¸ä¼¼
            same_class_mask = labels.unsqueeze(0) == labels.unsqueeze(1)
            same_class_mask = same_class_mask & (~eye_mask)
            
            if same_class_mask.sum() > 0:
                # æœ‰åŒç±»æ ·æœ¬
                pos_similarities = similarity_matrix[same_class_mask]
                neg_similarities = similarity_matrix[~same_class_mask & ~eye_mask]
                
                # ç®€åŒ–çš„å¯¹æ¯”æŸå¤±ï¼šæœ€å¤§åŒ–åŒç±»ç›¸ä¼¼åº¦ï¼Œæœ€å°åŒ–ä¸åŒç±»ç›¸ä¼¼åº¦
                if len(pos_similarities) > 0 and len(neg_similarities) > 0:
                    pos_loss = -torch.log(torch.sigmoid(pos_similarities)).mean()
                    neg_loss = -torch.log(torch.sigmoid(-neg_similarities)).mean()
                    contrastive_loss = (pos_loss + neg_loss) / 2
                else:
                    contrastive_loss = torch.tensor(0.0, device=device, requires_grad=True)
            else:
                contrastive_loss = torch.tensor(0.0, device=device, requires_grad=True)
        else:
            # æ ‡å‡†InfoNCEæŸå¤±è®¡ç®—
            for i in range(batch_size):
                positive_indices = positive_mask[i].nonzero(as_tuple=True)[0]
                
                if len(positive_indices) > 0:
                    # è·å–å½“å‰æ ·æœ¬çš„ç›¸ä¼¼åº¦åˆ†æ•°
                    sim_i = similarity_matrix[i]  # [batch_size]
                    
                    # ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œå‡å»æœ€å¤§å€¼
                    sim_i = sim_i - sim_i.max().detach()
                    
                    # å¯¹æ¯ä¸ªæ­£æ ·æœ¬è®¡ç®—æŸå¤±
                    for pos_idx in positive_indices:
                        pos_sim = sim_i[pos_idx]
                        
                        # æ‰€æœ‰æ ·æœ¬ï¼ˆé™¤è‡ªå·±ï¼‰ä½œä¸ºåˆ†æ¯
                        all_sims = torch.cat([sim_i[:i], sim_i[i+1:]])  # æ’é™¤è‡ªèº«
                        
                        # InfoNCEæŸå¤±
                        log_prob = pos_sim - torch.logsumexp(all_sims, dim=0)
                        losses.append(-log_prob)
            
            if losses:
                contrastive_loss = torch.stack(losses).mean()
            else:
                contrastive_loss = torch.tensor(0.0, device=device, requires_grad=True)
        
        # 7. æ·»åŠ æŸå¤±ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
        if hasattr(self, 'training') and self.training:
            # è®°å½•ä¸€äº›ç»Ÿè®¡ä¿¡æ¯
            with torch.no_grad():
                self._contrastive_stats = {
                    'avg_similarity': similarity_matrix[~eye_mask].mean().item(),
                    'positive_pairs': total_positive_pairs,
                    'effective_temperature': effective_temperature,
                    'loss_value': contrastive_loss.item()
                }
        
        return contrastive_loss
    
    def get_embeddings(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        """
        è·å–æ–‡æœ¬çš„åµŒå…¥è¡¨ç¤ºï¼ˆç”¨äºæ¨ç†ï¼‰
        
        Args:
            input_ids: è¾“å…¥token IDs
            attention_mask: æ³¨æ„åŠ›æ©ç 
            
        Returns:
            å½’ä¸€åŒ–çš„åµŒå…¥å‘é‡ [batch_size, contrastive_dim]
        """
        with torch.no_grad():
            outputs = self.forward(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_embeddings=True
            )
            return outputs['contrastive_embeddings']


class JointBertTrainer:
    """
    è”åˆBERTæ¨¡å‹è®­ç»ƒå™¨
    å®ç°è®ºæ–‡ä¸­çš„è”åˆæŸå¤±å‡½æ•°ï¼šL_joint = Î²_ft * L_MLM + (1 - Î²_ft) * L_InfoNCE
    """
    
    def __init__(self, 
                 model: JointBertModel,
                 beta_ft: float = 0.5,
                 device: str = 'cuda'):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model: è”åˆBERTæ¨¡å‹
            beta_ft: MLMå’Œå¯¹æ¯”å­¦ä¹ çš„æƒé‡å¹³è¡¡å‚æ•° Î²_ft âˆˆ [0, 1]
            device: è®¾å¤‡
        """
        self.model = model
        self.beta_ft = beta_ft
        self.device = device
        # è·å–æ¨¡å‹çš„temperatureå‚æ•°
        model_to_use = model.module if hasattr(model, 'module') else model
        self.temperature = model_to_use.temperature
        
        logger.info(f"åˆå§‹åŒ–JointBertTrainer - Î²_ft: {beta_ft}, temperature: {self.temperature}")
    
    def _compute_pairwise_contrastive_loss(self, 
                                          emb1: torch.Tensor, 
                                          emb2: torch.Tensor, 
                                          labels: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—pair-wiseå¯¹æ¯”å­¦ä¹ æŸå¤±
        
        å¯¹äºæ¯ä¸€å¯¹(emb1[i], emb2[i]):
        - å¦‚æœlabels[i]=1ï¼ˆæ­£æ ·æœ¬å¯¹ï¼‰ï¼šä½¿å®ƒä»¬ç›¸ä¼¼
        - å¦‚æœlabels[i]=0ï¼ˆè´Ÿæ ·æœ¬å¯¹ï¼‰ï¼šä½¿å®ƒä»¬ä¸ç›¸ä¼¼
        
        Args:
            emb1: ç¬¬ä¸€ç»„åµŒå…¥ [batch_size, dim]
            emb2: ç¬¬äºŒç»„åµŒå…¥ [batch_size, dim]
            labels: æ ‡ç­¾ [batch_size]ï¼Œ1=æ­£æ ·æœ¬å¯¹ï¼Œ0=è´Ÿæ ·æœ¬å¯¹
            
        Returns:
            å¯¹æ¯”å­¦ä¹ æŸå¤±
        """
        # è®¡ç®—æ¯å¯¹ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        # emb1å’Œemb2å·²ç»åœ¨è°ƒç”¨å‰å½’ä¸€åŒ–ï¼Œæ‰€ä»¥ç‚¹ç§¯=ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = torch.sum(emb1 * emb2, dim=1)  # [batch_size]
        
        # ç¼©æ”¾ç›¸ä¼¼åº¦ï¼ˆåº”ç”¨æ¸©åº¦å‚æ•°ï¼‰
        similarities = similarities / self.temperature
        
        # è®¡ç®—æŸå¤±
        # æ­£æ ·æœ¬å¯¹ï¼šæœ€å¤§åŒ–ç›¸ä¼¼åº¦ -> -log(sigmoid(sim))
        # è´Ÿæ ·æœ¬å¯¹ï¼šæœ€å°åŒ–ç›¸ä¼¼åº¦ -> -log(sigmoid(-sim)) = -log(1-sigmoid(sim))
        
        positive_mask = labels == 1
        negative_mask = labels == 0
        
        loss = 0.0
        n_pos = positive_mask.sum().item()
        n_neg = negative_mask.sum().item()
        
        if n_pos > 0:
            # æ­£æ ·æœ¬å¯¹æŸå¤±ï¼šå¸Œæœ›ç›¸ä¼¼åº¦é«˜
            pos_similarities = similarities[positive_mask]
            pos_loss = -torch.log(torch.sigmoid(pos_similarities) + 1e-10).mean()
            loss = loss + pos_loss
        
        if n_neg > 0:
            # è´Ÿæ ·æœ¬å¯¹æŸå¤±ï¼šå¸Œæœ›ç›¸ä¼¼åº¦ä½
            neg_similarities = similarities[negative_mask]
            neg_loss = -torch.log(torch.sigmoid(-neg_similarities) + 1e-10).mean()
            loss = loss + neg_loss
        
        # å¦‚æœæœ‰æ­£è´Ÿæ ·æœ¬ï¼Œå–å¹³å‡
        if n_pos > 0 and n_neg > 0:
            loss = loss / 2
        
        return loss
    
    def compute_joint_loss(self, 
                          mlm_loss: torch.Tensor,
                          contrastive_loss: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—è”åˆæŸå¤±ï¼šL_joint = Î²_ft * L_MLM + (1 - Î²_ft) * L_InfoNCE
        
        Args:
            mlm_loss: MLMæŸå¤±
            contrastive_loss: å¯¹æ¯”å­¦ä¹ æŸå¤±
            
        Returns:
            è”åˆæŸå¤±
        """
        joint_loss = self.beta_ft * mlm_loss + (1 - self.beta_ft) * contrastive_loss
        return joint_loss
    
    def forward_batch(self, 
                     mlm_batch: Dict[str, torch.Tensor],
                     contrastive_batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„è”åˆè®­ç»ƒ
        
        Args:
            mlm_batch: MLMæ‰¹æ¬¡æ•°æ®
            contrastive_batch: å¯¹æ¯”å­¦ä¹ æ‰¹æ¬¡æ•°æ®
            
        Returns:
            åŒ…å«æŸå¤±å’ŒæŒ‡æ ‡çš„å­—å…¸
        """
        results = {}
        
        # MLMå‰å‘ä¼ æ’­
        if mlm_batch:
            mlm_outputs = self.model(
                input_ids=mlm_batch['input_ids'],
                attention_mask=mlm_batch['attention_mask'],
                mlm_labels=mlm_batch['labels']
            )
            results['mlm_loss'] = mlm_outputs['mlm_loss']
            results['mlm_logits'] = mlm_outputs['mlm_logits']
        
        # å¯¹æ¯”å­¦ä¹ å‰å‘ä¼ æ’­
        if contrastive_batch:
            # å¤„ç†æˆå¯¹çš„è¾“å…¥
            batch_size = contrastive_batch['input_ids_1'].shape[0]
            
            # å¤„ç†DistributedDataParallelåŒ…è£…çš„æƒ…å†µ
            model_to_use = self.model.module if hasattr(self.model, 'module') else self.model
            
            # ç¬¬ä¸€ä¸ªæ–‡æœ¬
            outputs_1 = model_to_use(
                input_ids=contrastive_batch['input_ids_1'],
                attention_mask=contrastive_batch['attention_mask_1']
            )
            
            # ç¬¬äºŒä¸ªæ–‡æœ¬
            outputs_2 = model_to_use(
                input_ids=contrastive_batch['input_ids_2'],
                attention_mask=contrastive_batch['attention_mask_2']
            )
            
            # è·å–åµŒå…¥
            emb1 = outputs_1['contrastive_embeddings']  # [batch_size, dim]
            emb2 = outputs_2['contrastive_embeddings']  # [batch_size, dim]
            
            # å½’ä¸€åŒ–
            emb1 = F.normalize(emb1, p=2, dim=1)
            emb2 = F.normalize(emb2, p=2, dim=1)
            
            # ä½¿ç”¨pair-wiseå¯¹æ¯”å­¦ä¹ æŸå¤±
            labels = contrastive_batch['labels']  # [batch_size], 1=æ­£æ ·æœ¬å¯¹, 0=è´Ÿæ ·æœ¬å¯¹
            contrastive_loss = self._compute_pairwise_contrastive_loss(emb1, emb2, labels)
            results['contrastive_loss'] = contrastive_loss
        
        # è®¡ç®—è”åˆæŸå¤±
        if 'mlm_loss' in results and 'contrastive_loss' in results:
            joint_loss = self.compute_joint_loss(results['mlm_loss'], results['contrastive_loss'])
            results['joint_loss'] = joint_loss
        elif 'mlm_loss' in results:
            results['joint_loss'] = results['mlm_loss']
        elif 'contrastive_loss' in results:
            results['joint_loss'] = results['contrastive_loss']
        
        return results


def create_joint_bert_model(bert_model_path: str, 
                           contrastive_dim: int = 128,
                           temperature: float = 0.07) -> JointBertModel:
    """
    åˆ›å»ºè”åˆBERTæ¨¡å‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        bert_model_path: BERTæ¨¡å‹è·¯å¾„
        contrastive_dim: å¯¹æ¯”å­¦ä¹ åµŒå…¥ç»´åº¦
        temperature: æ¸©åº¦å‚æ•°
        
    Returns:
        JointBertModelå®ä¾‹
    """
    model = JointBertModel(
        bert_model_path=bert_model_path,
        contrastive_dim=contrastive_dim,
        temperature=temperature
    )
    
    return model


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºæ¨¡å‹
    model_path = "./bert-base-uncased"
    model = create_joint_bert_model(model_path)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = JointBertTrainer(model, beta_ft=0.5)
    
    print("âœ… è”åˆBERTæ¨¡å‹åˆ›å»ºæˆåŠŸï¼")
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°:")
    print(f"  - BERTéšè—ç»´åº¦: {model.hidden_size}")
    print(f"  - å¯¹æ¯”å­¦ä¹ ç»´åº¦: {model.contrastive_dim}")
    print(f"  - æ¸©åº¦å‚æ•°: {model.temperature}")
    print(f"  - æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
